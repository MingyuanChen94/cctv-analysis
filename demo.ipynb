{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCTV Analysis\n",
    "This notebook demonstrates the capabilities of our CCTV analysis system for multi-camera person tracking and analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ⚠️ torchvision==0.20 is incompatible with torch==2.4.\n",
      "Run 'pip install torchvision==0.19' to fix torchvision or 'pip install -U torch torchvision' to update both.\n",
      "For a full compatibility table see https://github.com/pytorch/vision#installation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <CAF361F5-1CAC-3EBE-9FC4-4B823D275CAA> /Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/chenm/miniconda3/envs/reid-env/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/Users/chenm/miniconda3/envs/reid-env/lib/python3.8/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchreid\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import scipy.spatial.distance as distance\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/chenm/Library/CloudStorage/OneDrive-UniversityofExeter/Documents/VISIONARY/Durham Experiment/test_data\n"
     ]
    }
   ],
   "source": [
    "# Set the working directory\n",
    "working_directory = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"CloudStorage\", \n",
    "                              \"OneDrive-UniversityofExeter\", \"Documents\", \"VISIONARY\", \n",
    "                              \"Durham Experiment\", \"test_data\")\n",
    "# working_directory = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"CloudStorage\", \n",
    "#                               \"OneDrive-UniversityofExeter\", \"Documents\", \"VISIONARY\", \n",
    "#                               \"Durham Experiment\", \"processed_data_3\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Camera_2_20241101.mp4', 'Camera_1_20241101.mp4']\n"
     ]
    }
   ],
   "source": [
    "# Get the .mp4 files in the folder\n",
    "mp4_files = list(Path(working_directory).glob(\"*.mp4\"))\n",
    "# Print the .mp4 files without showing the parent directories\n",
    "print([file.name for file in mp4_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera 1 files sorted by date: ['Camera_1_20241101.mp4']\n",
      "Camera 2 files sorted by date: ['Camera_2_20241101.mp4']\n"
     ]
    }
   ],
   "source": [
    "# Filter the files for Camera_1 and Camera_2\n",
    "camera_1_files = [file for file in mp4_files if file.name.startswith(\"Camera_1_\")]\n",
    "camera_2_files = [file for file in mp4_files if file.name.startswith(\"Camera_2_\")]\n",
    "\n",
    "# Sort the files by date extracted from the filename\n",
    "camera_1_files_sorted = sorted(camera_1_files, key=lambda x: x.stem.split('_')[-1])\n",
    "camera_2_files_sorted = sorted(camera_2_files, key=lambda x: x.stem.split('_')[-1])\n",
    "\n",
    "print(\"Camera 1 files sorted by date:\", [file.name for file in camera_1_files_sorted])\n",
    "print(\"Camera 2 files sorted by date:\", [file.name for file in camera_2_files_sorted])\n",
    "\n",
    "del mp4_files, camera_1_files, camera_2_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = camera_1_files_sorted[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initate Video Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackingState:\n",
    "    ACTIVE = 'active'          # Fully visible\n",
    "    OCCLUDED = 'occluded'      # Temporarily hidden\n",
    "    TENTATIVE = 'tentative'    # New track\n",
    "    LOST = 'lost'              # Missing too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackingState:\n",
    "    ACTIVE = 'active'          # Fully visible\n",
    "    OCCLUDED = 'occluded'      # Temporarily hidden\n",
    "    TENTATIVE = 'tentative'    # New track\n",
    "    LOST = 'lost'              # Missing too long\n",
    "\n",
    "class PersonTracker:\n",
    "    def __init__(self, video_path, output_dir=\"tracked_persons\"):\n",
    "        # Initialize YOLO model\n",
    "        self.detector = YOLO(\"yolo11x.pt\")\n",
    "        \n",
    "        # Initialize ReID model\n",
    "        self.reid_model = torchreid.models.build_model(\n",
    "            name='osnet_x1_0',\n",
    "            num_classes=1000,\n",
    "            pretrained=True\n",
    "        )\n",
    "        self.reid_model = self.reid_model.cuda() if torch.cuda.is_available() else self.reid_model\n",
    "        self.reid_model.eval()\n",
    "        \n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        self.active_tracks = {}  # Currently active tracks\n",
    "        self.person_features = {}  # Historical features for each ID\n",
    "        self.person_timestamps = {}  # Timestamp information\n",
    "        self.next_id = 0\n",
    "        \n",
    "        # Tracking parameters\n",
    "        self.similarity_threshold = 0.7\n",
    "        self.max_disappeared = self.fps * 2  # Max frames to keep track without detection\n",
    "        self.min_detection_confidence = 0.5\n",
    "        self.feature_weight = 0.4   # Weight for ReID features in matching\n",
    "        self.position_weight = 0.3  # Weight for absolute position (IoU)\n",
    "        self.motion_weight = 0.3    # Weight for relative motion prediction\n",
    "        \n",
    "    def extract_features(self, person_crop):\n",
    "        \"\"\"Extract ReID features from person crop\"\"\"\n",
    "        try:\n",
    "            # Preprocess image for ReID\n",
    "            img = cv2.resize(person_crop, (128, 256))\n",
    "            img = torch.from_numpy(img).float()\n",
    "            img = img.permute(2, 0, 1).unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = self.reid_model(img)\n",
    "            return features.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def calculate_box_center(self, box):\n",
    "        \"\"\"Calculate center point of a bounding box\"\"\"\n",
    "        return [(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]\n",
    "\n",
    "    def calculate_velocity(self, current_box, previous_box):\n",
    "        \"\"\"Calculate velocity vector between two boxes\"\"\"\n",
    "        current_center = self.calculate_box_center(current_box)\n",
    "        previous_center = self.calculate_box_center(previous_box)\n",
    "        return [current_center[0] - previous_center[0], \n",
    "                current_center[1] - previous_center[1]]\n",
    "\n",
    "    def predict_next_position(self, box, velocity):\n",
    "        \"\"\"Predict next position based on current position and velocity\"\"\"\n",
    "        center = self.calculate_box_center(box)\n",
    "        predicted_center = [center[0] + velocity[0], center[1] + velocity[1]]\n",
    "        width = box[2] - box[0]\n",
    "        height = box[3] - box[1]\n",
    "        return [predicted_center[0] - width/2, predicted_center[1] - height/2,\n",
    "                predicted_center[0] + width/2, predicted_center[1] + height/2]\n",
    "\n",
    "    def calculate_motion_similarity(self, current_boxes, tracked_boxes, tracked_velocities):\n",
    "        \"\"\"Calculate motion-based similarity\"\"\"\n",
    "        n_detections = len(current_boxes)\n",
    "        n_tracks = len(tracked_boxes)\n",
    "        motion_sim = np.zeros((n_detections, n_tracks))\n",
    "        \n",
    "        for i, current_box in enumerate(current_boxes):\n",
    "            current_center = self.calculate_box_center(current_box)\n",
    "            for j, (tracked_box, velocity) in enumerate(zip(tracked_boxes, tracked_velocities)):\n",
    "                # Predict where the tracked box should be\n",
    "                predicted_box = self.predict_next_position(tracked_box, velocity)\n",
    "                predicted_center = self.calculate_box_center(predicted_box)\n",
    "                \n",
    "                # Calculate distance between prediction and actual position\n",
    "                distance = np.sqrt(\n",
    "                    (current_center[0] - predicted_center[0])**2 +\n",
    "                    (current_center[1] - predicted_center[1])**2\n",
    "                )\n",
    "                # Convert distance to similarity (closer = more similar)\n",
    "                motion_sim[i, j] = np.exp(-distance / 100.0)  # 100 is a scaling factor\n",
    "                \n",
    "        return motion_sim\n",
    "\n",
    "    def detect_occlusion(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Detect if box1 is occluded by box2.\n",
    "        Returns: \n",
    "            - is_occluded (bool): True if box1 is occluded by box2\n",
    "            - occlusion_score (float): Degree of occlusion (0 to 1)\n",
    "        \"\"\"\n",
    "        # Calculate IoU\n",
    "        iou = self.calculate_iou(box1, box2)\n",
    "        \n",
    "        # Calculate centers and areas\n",
    "        center1 = self.calculate_box_center(box1)\n",
    "        center2 = self.calculate_box_center(box2)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        \n",
    "        # Calculate vertical position (y-coordinate)\n",
    "        y1 = box1[3]  # bottom of box1\n",
    "        y2 = box2[3]  # bottom of box2\n",
    "        \n",
    "        # Factors that suggest box1 is behind box2:\n",
    "        # 1. Significant overlap\n",
    "        overlap_factor = 1.0 if iou > 0.3 else 0.0\n",
    "        \n",
    "        # 2. Box2 is closer to camera (generally larger and lower in frame)\n",
    "        size_factor = 1.0 if area2 > area1 else 0.0\n",
    "        position_factor = 1.0 if y2 > y1 else 0.0\n",
    "        \n",
    "        # 3. Box1 is partially contained within box2\n",
    "        contained_horizontally = (\n",
    "            (box1[0] > box2[0] and box1[0] < box2[2]) or\n",
    "            (box1[2] > box2[0] and box1[2] < box2[2])\n",
    "        )\n",
    "        contained_vertically = (\n",
    "            (box1[1] > box2[1] and box1[1] < box2[3]) or\n",
    "            (box1[3] > box2[1] and box1[3] < box2[3])\n",
    "        )\n",
    "        containment_factor = 1.0 if (contained_horizontally and contained_vertically) else 0.0\n",
    "        \n",
    "        # Calculate occlusion score (weighted combination of factors)\n",
    "        occlusion_score = (\n",
    "            0.4 * overlap_factor +\n",
    "            0.2 * size_factor +\n",
    "            0.2 * position_factor +\n",
    "            0.2 * containment_factor\n",
    "        )\n",
    "        \n",
    "        # Determine if occluded based on score threshold\n",
    "        is_occluded = occlusion_score > 0.5\n",
    "        \n",
    "        return is_occluded, occlusion_score\n",
    "\n",
    "    def calculate_similarity_matrix(self, current_features, current_boxes, tracked_features, tracked_boxes):\n",
    "        \"\"\"Calculate similarity matrix combining appearance, position, and motion\"\"\"\n",
    "        n_detections = len(current_features)\n",
    "        n_tracks = len(tracked_features)\n",
    "        \n",
    "        if n_detections == 0 or n_tracks == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        # Calculate appearance similarity\n",
    "        appearance_sim = 1 - distance.cdist(\n",
    "            np.array([f.flatten() for f in current_features]), \n",
    "            np.array([f.flatten() for f in tracked_features]), \n",
    "            metric='cosine'\n",
    "        )\n",
    "        \n",
    "        # Calculate position similarity using IoU\n",
    "        position_sim = np.zeros((n_detections, n_tracks))\n",
    "        for i, box1 in enumerate(current_boxes):\n",
    "            for j, box2 in enumerate(tracked_boxes):\n",
    "                position_sim[i, j] = self.calculate_iou(box1, box2)\n",
    "        \n",
    "        # Calculate velocities for tracked objects\n",
    "        tracked_velocities = []\n",
    "        for track_id in list(self.active_tracks.keys())[:n_tracks]:\n",
    "            if 'previous_box' in self.active_tracks[track_id]:\n",
    "                velocity = self.calculate_velocity(\n",
    "                    self.active_tracks[track_id]['box'],\n",
    "                    self.active_tracks[track_id]['previous_box']\n",
    "                )\n",
    "            else:\n",
    "                velocity = [0, 0]  # No velocity for new tracks\n",
    "            tracked_velocities.append(velocity)\n",
    "        \n",
    "        # Calculate motion similarity\n",
    "        motion_sim = self.calculate_motion_similarity(current_boxes, tracked_boxes, tracked_velocities)\n",
    "        \n",
    "        # Combine all similarities\n",
    "        similarity_matrix = (\n",
    "            self.feature_weight * appearance_sim + \n",
    "            self.position_weight * position_sim +\n",
    "            self.motion_weight * motion_sim\n",
    "        )\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_iou(box1, box2):\n",
    "        \"\"\"Calculate IoU between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / (union + 1e-6)\n",
    "    \n",
    "    def update_feature_history(self, track_id, features):\n",
    "        \"\"\"Maintain rolling window of recent features\"\"\"\n",
    "        self.appearance_history[track_id].append(features)\n",
    "        if len(self.appearance_history[track_id]) > self.max_history_length:\n",
    "            self.appearance_history[track_id].pop(0)\n",
    "            \n",
    "        # Update feature representation using exponential moving average\n",
    "        if track_id in self.person_features:\n",
    "            alpha = 0.7  # Weight for historical features\n",
    "            current_features = self.person_features[track_id]\n",
    "            updated_features = alpha * current_features + (1 - alpha) * features\n",
    "            self.person_features[track_id] = updated_features\n",
    "        else:\n",
    "            self.person_features[track_id] = features\n",
    "\n",
    "    def recover_lost_tracklet(self, features, current_box, frame_time):\n",
    "        \"\"\"Attempt to recover lost tracks\"\"\"\n",
    "        best_match_id = None\n",
    "        best_match_score = 0\n",
    "        \n",
    "        # Check recently lost tracks\n",
    "        lost_tracks_to_remove = []\n",
    "        for lost_id, lost_info in self.lost_tracks.items():\n",
    "            # Skip if lost track is too old\n",
    "            if frame_time - lost_info['last_seen'] > self.max_lost_age:\n",
    "                lost_tracks_to_remove.append(lost_id)\n",
    "                continue\n",
    "                \n",
    "            # Calculate appearance similarity\n",
    "            lost_features = lost_info['features']\n",
    "            appearance_sim = 1 - distance.cosine(features.flatten(), lost_features.flatten())\n",
    "            \n",
    "            # Calculate position similarity based on predicted movement\n",
    "            predicted_box = self.predict_next_position(\n",
    "                lost_info['box'],\n",
    "                lost_info['velocity']\n",
    "            )\n",
    "            position_sim = self.calculate_iou(current_box, predicted_box)\n",
    "            \n",
    "            # Combine similarities\n",
    "            match_score = (\n",
    "                self.feature_weight * appearance_sim +\n",
    "                self.position_weight * position_sim\n",
    "            )\n",
    "            \n",
    "            # Check temporal consistency\n",
    "            if match_score > 0.6 and match_score > best_match_score:\n",
    "                best_match_score = match_score\n",
    "                best_match_id = lost_id\n",
    "        \n",
    "        # Clean up old lost tracks\n",
    "        for lost_id in lost_tracks_to_remove:\n",
    "            del self.lost_tracks[lost_id]\n",
    "            \n",
    "        return best_match_id if best_match_score > 0.6 else None\n",
    "\n",
    "    def update_tracks(self, frame, detections, frame_time):\n",
    "        \"\"\"Update tracks with new detections\"\"\"\n",
    "        current_boxes = []\n",
    "        current_features = []\n",
    "        \n",
    "        # Process new detections\n",
    "        for box, conf in detections:\n",
    "            if conf < self.min_detection_confidence:\n",
    "                continue\n",
    "                \n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            if person_crop.size == 0:\n",
    "                continue\n",
    "                \n",
    "            features = self.extract_features(person_crop)\n",
    "            if features is not None:\n",
    "                current_boxes.append([x1, y1, x2, y2])\n",
    "                current_features.append(features)\n",
    "        \n",
    "        # Get tracked boxes and features\n",
    "        tracked_boxes = []\n",
    "        tracked_features = []\n",
    "        tracked_ids = []\n",
    "        \n",
    "        for track_id, track_info in self.active_tracks.items():\n",
    "            tracked_boxes.append(track_info['box'])\n",
    "            tracked_features.append(track_info['features'])\n",
    "            tracked_ids.append(track_id)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = self.calculate_similarity_matrix(\n",
    "            current_features, current_boxes,\n",
    "            tracked_features, tracked_boxes\n",
    "        )\n",
    "        \n",
    "        # Perform matching\n",
    "        matched_indices = []\n",
    "        if similarity_matrix.size > 0:\n",
    "            row_ind, col_ind = linear_sum_assignment(-similarity_matrix)\n",
    "            matched_indices = list(zip(row_ind, col_ind))\n",
    "        \n",
    "        # Process matches\n",
    "        unmatched_detections = []\n",
    "        matched_track_ids = set()\n",
    "        \n",
    "        for detection_idx, track_idx in matched_indices:\n",
    "            similarity = similarity_matrix[detection_idx, track_idx]\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                track_id = tracked_ids[track_idx]\n",
    "                matched_track_ids.add(track_id)\n",
    "                \n",
    "                # Update track\n",
    "                self.active_tracks[track_id].update({\n",
    "                    'previous_box': self.active_tracks[track_id]['box'],\n",
    "                    'box': current_boxes[detection_idx],\n",
    "                    'features': current_features[detection_idx],\n",
    "                    'last_seen': frame_time,\n",
    "                    'disappeared': 0\n",
    "                })\n",
    "                \n",
    "                # Update timestamps\n",
    "                self.person_timestamps[track_id]['last_appearance'] = frame_time\n",
    "                \n",
    "                # Save person image\n",
    "                self.save_person_image(track_id, \n",
    "                    frame[current_boxes[detection_idx][1]:current_boxes[detection_idx][3],\n",
    "                          current_boxes[detection_idx][0]:current_boxes[detection_idx][2]])\n",
    "            else:\n",
    "                unmatched_detections.append(detection_idx)\n",
    "        \n",
    "        # Add unmatched detections as new tracks\n",
    "        for detection_idx in range(len(current_features)):\n",
    "            if not any(detection_idx == m[0] for m in matched_indices):\n",
    "                new_id = self.next_id\n",
    "                self.next_id += 1\n",
    "                \n",
    "                self.active_tracks[new_id] = {\n",
    "                    'state': TrackingState.TENTATIVE,\n",
    "                    'occlusion_counter': 0,\n",
    "                    'box': current_boxes[detection_idx],\n",
    "                    'features': current_features[detection_idx],\n",
    "                    'last_seen': frame_time,\n",
    "                    'disappeared': 0,\n",
    "                    'velocity': [0, 0]  # Initialize velocity for new tracks\n",
    "                }\n",
    "                \n",
    "                self.person_features[new_id] = [current_features[detection_idx]]\n",
    "                self.person_timestamps[new_id] = {\n",
    "                    'first_appearance': frame_time,\n",
    "                    'last_appearance': frame_time\n",
    "                }\n",
    "                \n",
    "                # Save person image\n",
    "                self.save_person_image(new_id, \n",
    "                    frame[current_boxes[detection_idx][1]:current_boxes[detection_idx][3],\n",
    "                          current_boxes[detection_idx][0]:current_boxes[detection_idx][2]])\n",
    "        \n",
    "        # Update disappeared tracks\n",
    "        current_time = frame_time\n",
    "        tracks_to_remove = []\n",
    "        \n",
    "        for track_id in self.active_tracks:\n",
    "            if track_id not in matched_track_ids:\n",
    "                self.active_tracks[track_id]['disappeared'] += 1\n",
    "                if self.active_tracks[track_id]['disappeared'] > self.max_disappeared:\n",
    "                    tracks_to_remove.append(track_id)\n",
    "        \n",
    "        # Remove old tracks\n",
    "        for track_id in tracks_to_remove:\n",
    "            del self.active_tracks[track_id]\n",
    "    \n",
    "    def save_person_image(self, person_id, frame):\n",
    "        \"\"\"Save person image to output directory\"\"\"\n",
    "        person_dir = os.path.join(self.output_dir, f\"person_{person_id}\")\n",
    "        os.makedirs(person_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        cv2.imwrite(os.path.join(person_dir, f\"{timestamp}.jpg\"), frame)\n",
    "    \n",
    "    def process_video(self):\n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame_time = frame_count / self.fps\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Detect persons using YOLO\n",
    "            results = self.detector(frame, classes=[0])  # class 0 is person\n",
    "            \n",
    "            # Process detections\n",
    "            detections = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes.cpu().numpy()\n",
    "                for box in boxes:\n",
    "                    detections.append((box.xyxy[0], box.conf[0]))\n",
    "            \n",
    "            # Update tracking\n",
    "            self.update_tracks(frame, detections, frame_time)\n",
    "            \n",
    "            # Visualize results\n",
    "            for track_id, track_info in self.active_tracks.items():\n",
    "                box = track_info['box']\n",
    "                cv2.rectangle(frame, (int(box[0]), int(box[1])), \n",
    "                            (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"ID: {track_id}\", \n",
    "                          (int(box[0]), int(box[1])-10),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display frame\n",
    "            cv2.imshow('Tracking', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return self.generate_report()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate tracking report\"\"\"\n",
    "        report = {\n",
    "            'total_unique_persons': self.next_id,\n",
    "            'person_details': {}\n",
    "        }\n",
    "        \n",
    "        for person_id in self.person_timestamps.keys():\n",
    "            report['person_details'][person_id] = {\n",
    "                'first_appearance': self.person_timestamps[person_id]['first_appearance'],\n",
    "                'last_appearance': self.person_timestamps[person_id]['last_appearance'],\n",
    "                'duration': self.person_timestamps[person_id]['last_appearance'] - \n",
    "                          self.person_timestamps[person_id]['first_appearance'],\n",
    "                'image_path': os.path.join(self.output_dir, f\"person_{person_id}\")\n",
    "            }\n",
    "            \n",
    "        return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded imagenet pretrained weights from \"/Users/chenm/.cache/torch/checkpoints/osnet_x1_0_imagenet.pth\"\n",
      "\n",
      "0: 384x640 8 persons, 186.6ms\n",
      "Speed: 1.7ms preprocess, 186.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 190.8ms\n",
      "Speed: 1.5ms preprocess, 190.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 197.0ms\n",
      "Speed: 1.5ms preprocess, 197.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 14:56:27.974 python[3686:15990045] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2024-12-28 14:56:27.974 python[3686:15990045] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 384x640 7 persons, 193.8ms\n",
      "Speed: 1.4ms preprocess, 193.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 192.4ms\n",
      "Speed: 1.4ms preprocess, 192.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 199.2ms\n",
      "Speed: 1.4ms preprocess, 199.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 178.8ms\n",
      "Speed: 1.3ms preprocess, 178.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 182.7ms\n",
      "Speed: 1.4ms preprocess, 182.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 176.4ms\n",
      "Speed: 1.4ms preprocess, 176.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 181.7ms\n",
      "Speed: 1.3ms preprocess, 181.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 182.0ms\n",
      "Speed: 1.4ms preprocess, 182.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 177.5ms\n",
      "Speed: 1.5ms preprocess, 177.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 215.9ms\n",
      "Speed: 1.3ms preprocess, 215.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 185.3ms\n",
      "Speed: 1.4ms preprocess, 185.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 175.5ms\n",
      "Speed: 1.4ms preprocess, 175.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 188.7ms\n",
      "Speed: 1.4ms preprocess, 188.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 183.5ms\n",
      "Speed: 1.4ms preprocess, 183.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 177.9ms\n",
      "Speed: 1.4ms preprocess, 177.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 176.0ms\n",
      "Speed: 1.3ms preprocess, 176.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 175.1ms\n",
      "Speed: 1.3ms preprocess, 175.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 177.9ms\n",
      "Speed: 1.3ms preprocess, 177.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 176.5ms\n",
      "Speed: 1.2ms preprocess, 176.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 182.9ms\n",
      "Speed: 1.4ms preprocess, 182.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 179.8ms\n",
      "Speed: 1.4ms preprocess, 179.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 177.5ms\n",
      "Speed: 1.2ms preprocess, 177.5ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 170.7ms\n",
      "Speed: 1.3ms preprocess, 170.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 174.1ms\n",
      "Speed: 1.3ms preprocess, 174.1ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 174.9ms\n",
      "Speed: 1.3ms preprocess, 174.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 176.6ms\n",
      "Speed: 1.3ms preprocess, 176.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 170.6ms\n",
      "Speed: 1.4ms preprocess, 170.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 179.7ms\n",
      "Speed: 1.4ms preprocess, 179.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 173.2ms\n",
      "Speed: 1.2ms preprocess, 173.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 173.8ms\n",
      "Speed: 1.3ms preprocess, 173.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 176.3ms\n",
      "Speed: 1.2ms preprocess, 176.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 165.0ms\n",
      "Speed: 1.2ms preprocess, 165.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 173.5ms\n",
      "Speed: 1.4ms preprocess, 173.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 179.1ms\n",
      "Speed: 1.3ms preprocess, 179.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 173.6ms\n",
      "Speed: 1.2ms preprocess, 173.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 180.3ms\n",
      "Speed: 1.2ms preprocess, 180.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 176.7ms\n",
      "Speed: 1.4ms preprocess, 176.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 174.7ms\n",
      "Speed: 1.3ms preprocess, 174.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 183.3ms\n",
      "Speed: 1.4ms preprocess, 183.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 174.0ms\n",
      "Speed: 1.4ms preprocess, 174.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 174.8ms\n",
      "Speed: 1.2ms preprocess, 174.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 185.6ms\n",
      "Speed: 1.3ms preprocess, 185.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 10 persons, 180.7ms\n",
      "Speed: 1.2ms preprocess, 180.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 201.1ms\n",
      "Speed: 1.5ms preprocess, 201.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 203.0ms\n",
      "Speed: 1.4ms preprocess, 203.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 177.2ms\n",
      "Speed: 1.3ms preprocess, 177.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 179.0ms\n",
      "Speed: 1.2ms preprocess, 179.0ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 184.6ms\n",
      "Speed: 1.2ms preprocess, 184.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 182.1ms\n",
      "Speed: 1.2ms preprocess, 182.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 182.3ms\n",
      "Speed: 1.4ms preprocess, 182.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 175.8ms\n",
      "Speed: 1.2ms preprocess, 175.8ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 170.7ms\n",
      "Speed: 1.4ms preprocess, 170.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 177.8ms\n",
      "Speed: 1.4ms preprocess, 177.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 207.4ms\n",
      "Speed: 1.6ms preprocess, 207.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 194.7ms\n",
      "Speed: 1.4ms preprocess, 194.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 197.3ms\n",
      "Speed: 1.4ms preprocess, 197.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 170.0ms\n",
      "Speed: 1.3ms preprocess, 170.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 11 persons, 169.3ms\n",
      "Speed: 1.3ms preprocess, 169.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 183.9ms\n",
      "Speed: 1.3ms preprocess, 183.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 182.5ms\n",
      "Speed: 1.2ms preprocess, 182.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 persons, 182.2ms\n",
      "Speed: 1.5ms preprocess, 182.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 175.6ms\n",
      "Speed: 1.3ms preprocess, 175.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 170.6ms\n",
      "Speed: 1.3ms preprocess, 170.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 180.9ms\n",
      "Speed: 1.2ms preprocess, 180.9ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 persons, 178.4ms\n",
      "Speed: 1.2ms preprocess, 178.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 6 persons, 179.3ms\n",
      "Speed: 1.4ms preprocess, 179.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 7 persons, 176.4ms\n",
      "Speed: 1.2ms preprocess, 176.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 166.3ms\n",
      "Speed: 1.4ms preprocess, 166.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 179.8ms\n",
      "Speed: 1.3ms preprocess, 179.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 182.1ms\n",
      "Speed: 1.2ms preprocess, 182.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 202.9ms\n",
      "Speed: 1.5ms preprocess, 202.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 183.3ms\n",
      "Speed: 1.4ms preprocess, 183.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 190.2ms\n",
      "Speed: 1.4ms preprocess, 190.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 211.6ms\n",
      "Speed: 1.4ms preprocess, 211.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 225.2ms\n",
      "Speed: 1.7ms preprocess, 225.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 253.6ms\n",
      "Speed: 1.4ms preprocess, 253.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 191.4ms\n",
      "Speed: 1.5ms preprocess, 191.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 9 persons, 184.4ms\n",
      "Speed: 1.5ms preprocess, 184.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 188.2ms\n",
      "Speed: 1.2ms preprocess, 188.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 8 persons, 202.0ms\n",
      "Speed: 1.3ms preprocess, 202.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m video_path \u001b[38;5;241m=\u001b[39m video_path  \u001b[38;5;66;03m# Replace with your video path\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tracker \u001b[38;5;241m=\u001b[39m PersonTracker(video_path)\n\u001b[0;32m----> 3\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal unique persons detected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_unique_persons\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m person_id, details \u001b[38;5;129;01min\u001b[39;00m report[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson_details\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[0;32mIn[16], line 405\u001b[0m, in \u001b[0;36mPersonTracker.process_video\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# Detect persons using YOLO\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# class 0 is person\u001b[39;00m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Process detections\u001b[39;00m\n\u001b[1;32m    408\u001b[0m detections \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:173\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:259\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m--> 259\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/engine/predictor.py:143\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[0;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    139\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    142\u001b[0m )\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/autobackend.py:524\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[0;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[0;32m--> 524\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:112\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:130\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[0;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/tasks.py:151\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[0;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 151\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    152\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36mC2f.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:238\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m    237\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 238\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:263\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3(torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(x)), \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/reid-env/lib/python3.8/site-packages/ultralytics/nn/modules/block.py:347\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_path = video_path  # Replace with your video path\n",
    "tracker = PersonTracker(video_path)\n",
    "report = tracker.process_video()\n",
    "print(f\"Total unique persons detected: {report['total_unique_persons']}\")\n",
    "for person_id, details in report['person_details'].items():\n",
    "    print(f\"\\nPerson ID: {person_id}\")\n",
    "    print(f\"First appearance: {details['first_appearance']:.2f}s\")\n",
    "    print(f\"Last appearance: {details['last_appearance']:.2f}s\")\n",
    "    print(f\"Duration in video: {details['duration']:.2f}s\")\n",
    "    print(f\"Images saved in: {details['image_path']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reid-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
