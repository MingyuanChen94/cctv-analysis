{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCTV Analysis\n",
    "This notebook demonstrates the capabilities of our CCTV analysis system for multi-camera person tracking and analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our modules\n",
    "from cctv_analysis.detector import PersonDetector\n",
    "from cctv_analysis.tracker import PersonTracker\n",
    "from cctv_analysis.reid import PersonReID\n",
    "from cctv_analysis.matcher import CameraPersonMatcher\n",
    "from cctv_analysis.demographics import DemographicAnalyzer\n",
    "from cctv_analysis.utils.visualization import Visualizer\n",
    "from cctv_analysis.utils.metrics import TrackingMetrics, MultiCameraMetrics, DemographicMetrics\n",
    "from cctv_analysis.utils.config import ConfigLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. System Setup\n",
    "First, let's initialize our models and configuration. We're using:\n",
    "- YOLOv8x6 for person detection\n",
    "- OSNet for person re-identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = ConfigLoader(\"configs/models.yaml\")\n",
    "model_config = config.get_model_config()\n",
    "tracking_config = config.get_tracking_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize detector with GPU acceleration\n",
    "detector = PersonDetector(\n",
    "    model_path=\"models/detector/yolov8x6.pt\",\n",
    "    conf_thresh=0.5\n",
    ")\n",
    "print(f\"Using device: {detector.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tracker with our configured settings\n",
    "tracker = PersonTracker(\n",
    "    reid_model=reid_model,\n",
    "    max_age=tracking_config.max_age,\n",
    "    min_hits=tracking_config.min_hits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize visualizer for displaying results\n",
    "visualizer = Visualizer(num_colors=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single Camera Tracking Demo\n",
    "Let's start by demonstrating how our system tracks people in a single camera view. We'll process a video \n",
    "and display the results frame by frame, showing how the tracker maintains consistent identities even when \n",
    "people move around or are temporarily occluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_camera(video_path: str, max_frames: int = 100, display_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Process a single camera video feed and display tracking results.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to the input video file\n",
    "        max_frames: Maximum number of frames to process\n",
    "        display_interval: Show every nth frame\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "    processing_metrics = {\n",
    "        'total_detections': 0,\n",
    "        'average_confidence': 0,\n",
    "        'unique_tracks': set()\n",
    "    }\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert BGR to RGB for processing\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Step 1: Detect people in the frame\n",
    "        detections = detector.detect(frame_rgb)\n",
    "        processing_metrics['total_detections'] += len(detections)\n",
    "        if detections:\n",
    "            processing_metrics['average_confidence'] += sum(\n",
    "                conf for _, conf in detections)\n",
    "\n",
    "        # Step 2: Update tracking with new detections\n",
    "        tracks = tracker.update(frame_rgb, detections)\n",
    "        for track in tracks:\n",
    "            processing_metrics['unique_tracks'].add(track.track_id)\n",
    "\n",
    "        # Display results at specified intervals\n",
    "        if frame_count % display_interval == 0:\n",
    "            vis_frame = visualizer.draw_tracks(\n",
    "                frame_rgb,\n",
    "                [(t.track_id, t.bbox) for t in tracks]\n",
    "            )\n",
    "\n",
    "            # Add frame information overlay\n",
    "            info_text = [\n",
    "                f\"Frame: {frame_count}\",\n",
    "                f\"Active Tracks: {len(tracks)}\",\n",
    "                f\"Total Unique IDs: {len(processing_metrics['unique_tracks'])}\"\n",
    "            ]\n",
    "\n",
    "            # Display frame with tracking results\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            plt.imshow(vis_frame)\n",
    "            plt.title(\"Person Tracking Results\", pad=20)\n",
    "\n",
    "            # Add metrics text\n",
    "            for i, text in enumerate(info_text):\n",
    "                plt.text(10, 20 + i * 20, text, color='white',\n",
    "                         backgroundcolor='black', fontsize=10)\n",
    "\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Calculate and display final metrics\n",
    "    if processing_metrics['total_detections'] > 0:\n",
    "        avg_conf = processing_metrics['average_confidence'] / \\\n",
    "            processing_metrics['total_detections']\n",
    "    else:\n",
    "        avg_conf = 0\n",
    "\n",
    "    print(\"\\nProcessing Summary:\")\n",
    "    print(f\"Total frames processed: {frame_count}\")\n",
    "    print(\n",
    "        f\"Average detections per frame: {processing_metrics['total_detections']/frame_count:.2f}\")\n",
    "    print(f\"Average detection confidence: {avg_conf:.3f}\")\n",
    "    print(f\"Total unique tracks: {len(processing_metrics['unique_tracks'])}\")\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Camera Tracking Demo\n",
    "Now let's demonstrate the system's ability to track people across multiple camera views. This is more \n",
    "challenging as we need to maintain consistent identities even when people disappear from one camera and \n",
    "reappear in another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our cross-camera matcher\n",
    "matcher = CameraPersonMatcher(\n",
    "    reid_model=reid_model,\n",
    "    matching_threshold=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_multiple_cameras(video_paths: dict, max_frames: int = 100, display_interval: int = 5):\n",
    "    \"\"\"\n",
    "    Process multiple synchronized video feeds and demonstrate cross-camera tracking.\n",
    "\n",
    "    Args:\n",
    "        video_paths: Dictionary mapping camera IDs to video file paths\n",
    "        max_frames: Maximum number of frames to process\n",
    "        display_interval: Show every nth frame\n",
    "    \"\"\"\n",
    "    # Open all video captures\n",
    "    captures = {\n",
    "        cam_id: cv2.VideoCapture(path)\n",
    "        for cam_id, path in video_paths.items()\n",
    "    }\n",
    "\n",
    "    # Initialize separate trackers for each camera\n",
    "    trackers = {\n",
    "        cam_id: PersonTracker(reid_model=reid_model)\n",
    "        for cam_id in video_paths.keys()\n",
    "    }\n",
    "\n",
    "    # Initialize metrics tracking\n",
    "    cross_camera_metrics = {\n",
    "        'global_ids': set(),\n",
    "        'transitions': 0,\n",
    "        'camera_appearances': defaultdict(int)\n",
    "    }\n",
    "\n",
    "    frame_count = 0\n",
    "    while frame_count < max_frames:\n",
    "        current_time = datetime.now()\n",
    "        frames = {}\n",
    "        detections = {}\n",
    "        tracks = {}\n",
    "\n",
    "        # Process each camera feed\n",
    "        for cam_id, cap in captures.items():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames[cam_id] = frame_rgb\n",
    "\n",
    "            # Detect and track people in this camera\n",
    "            dets = detector.detect(frame_rgb)\n",
    "            detections[cam_id] = dets\n",
    "            tracks[cam_id] = trackers[cam_id].update(frame_rgb, dets)\n",
    "\n",
    "            # Update global matches\n",
    "            for track in tracks[cam_id]:\n",
    "                # Get global ID for this track\n",
    "                global_id = matcher.update(\n",
    "                    camera_id=cam_id,\n",
    "                    track_id=track.track_id,\n",
    "                    feature=track.feature,\n",
    "                    timestamp=current_time\n",
    "                )\n",
    "\n",
    "                # Update metrics\n",
    "                cross_camera_metrics['global_ids'].add(global_id)\n",
    "                cross_camera_metrics['camera_appearances'][cam_id] += 1\n",
    "\n",
    "        # Display results at specified intervals\n",
    "        if frame_count % display_interval == 0:\n",
    "            # Get visualization with global IDs\n",
    "            vis_frames = visualizer.draw_multicamera_matches(\n",
    "                frames=frames,\n",
    "                matches={cam_id: [(t.track_id, t.bbox) for t in cam_tracks]\n",
    "                         for cam_id, cam_tracks in tracks.items()},\n",
    "                global_ids={cam_id: {t.track_id: matcher.get_global_id(cam_id, t.track_id)\n",
    "                                     for t in cam_tracks}\n",
    "                            for cam_id, cam_tracks in tracks.items()}\n",
    "            )\n",
    "\n",
    "            # Create multi-camera display\n",
    "            plt.figure(figsize=(20, 5 * len(captures)))\n",
    "            for idx, (cam_id, frame) in enumerate(vis_frames.items(), 1):\n",
    "                plt.subplot(len(captures), 1, idx)\n",
    "                plt.imshow(frame)\n",
    "                plt.title(\n",
    "                    f\"Camera {cam_id} - Active Tracks: {len(tracks[cam_id])}\")\n",
    "                plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Display final cross-camera statistics\n",
    "    print(\"\\nCross-Camera Tracking Summary:\")\n",
    "    print(\n",
    "        f\"Total unique people tracked: {len(cross_camera_metrics['global_ids'])}\")\n",
    "    print(\"\\nCamera Appearances:\")\n",
    "    for cam_id, count in cross_camera_metrics['camera_appearances'].items():\n",
    "        print(f\"Camera {cam_id}: {count} total detections\")\n",
    "\n",
    "    # Get popular transition paths\n",
    "    popular_paths = matcher.get_popular_paths(top_k=5)\n",
    "    print(\"\\nMost Common Camera Transitions:\")\n",
    "    for (cam1, cam2), count in popular_paths:\n",
    "        print(f\"Camera {cam1} → Camera {cam2}: {count} transitions\")\n",
    "\n",
    "    # Release all video captures\n",
    "    for cap in captures.values():\n",
    "        cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Demographic Analysis Demo\n",
    "Let's demonstrate how our system can analyze the demographics of tracked individuals over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our demographic analyzer\n",
    "demographic_analyzer = DemographicAnalyzer(\n",
    "    gender_model_path=\"models/demographics/gender_model.pth\",\n",
    "    age_model_path=\"models/demographics/age_model.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_demographics_over_time(video_path: str, max_frames: int = 100,\n",
    "                                   analysis_interval: int = 10):\n",
    "    \"\"\"\n",
    "    Demonstrate demographic analysis capabilities on a video stream.\n",
    "\n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        max_frames: Maximum frames to process\n",
    "        analysis_interval: Perform demographic analysis every n frames\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = 0\n",
    "\n",
    "    # Initialize metrics collectors\n",
    "    demographic_metrics = DemographicMetrics()\n",
    "    tracking_metrics = TrackingMetrics()\n",
    "\n",
    "    # Store historical data for visualization\n",
    "    temporal_data = {\n",
    "        'timestamps': [],\n",
    "        'gender_ratios': [],\n",
    "        'age_distributions': []\n",
    "    }\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        current_time = datetime.now()\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect and track people\n",
    "        detections = detector.detect(frame_rgb)\n",
    "        tracks = tracker.update(frame_rgb, detections)\n",
    "\n",
    "        # Perform demographic analysis at intervals\n",
    "        if frame_count % analysis_interval == 0:\n",
    "            frame_demographics = {}\n",
    "\n",
    "            # Analyze each tracked person\n",
    "            for track in tracks:\n",
    "                x1, y1, x2, y2 = track.bbox\n",
    "                person_patch = frame_rgb[y1:y2, x1:x2]\n",
    "\n",
    "                demo_info = demographic_analyzer.analyze_person(person_patch)\n",
    "                if demo_info:\n",
    "                    frame_demographics[track.track_id] = demo_info\n",
    "                    demographic_metrics.update(\n",
    "                        timestamp=current_time,\n",
    "                        gender=demo_info.gender,\n",
    "                        age_group=demo_info.age_group\n",
    "                    )\n",
    "\n",
    "            # Update temporal data\n",
    "            distribution = demographic_metrics.get_distribution()\n",
    "            temporal_data['timestamps'].append(current_time)\n",
    "            temporal_data['gender_ratios'].append(distribution['gender'])\n",
    "            temporal_data['age_distributions'].append(distribution['age'])\n",
    "\n",
    "            # Visualize current frame with demographic information\n",
    "            vis_frame = visualizer.draw_tracks_with_demographics(\n",
    "                frame_rgb,\n",
    "                [(t.track_id, t.bbox) for t in tracks],\n",
    "                frame_demographics\n",
    "            )\n",
    "\n",
    "            # Display results\n",
    "            plt.figure(figsize=(15, 10))\n",
    "\n",
    "            # Main frame with tracking and demographics\n",
    "            plt.subplot(2, 1, 1)\n",
    "            plt.imshow(vis_frame)\n",
    "            plt.title(f\"Frame {frame_count} - Demographic Analysis\")\n",
    "            plt.axis('off')\n",
    "\n",
    "            # Current demographic distribution\n",
    "            plt.subplot(2, 2, 3)\n",
    "            gender_dist = distribution['gender']\n",
    "            plt.bar(gender_dist.keys(), gender_dist.values())\n",
    "            plt.title(\"Gender Distribution\")\n",
    "            plt.ylabel(\"Proportion\")\n",
    "\n",
    "            plt.subplot(2, 2, 4)\n",
    "            age_dist = distribution['age']\n",
    "            plt.bar(age_dist.keys(), age_dist.values())\n",
    "            plt.title(\"Age Distribution\")\n",
    "            plt.xticks(rotation=45)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    # Display final analysis\n",
    "    print(\"\\nDemographic Analysis Summary:\")\n",
    "    final_dist = demographic_metrics.get_distribution()\n",
    "\n",
    "    print(\"\\nOverall Gender Distribution:\")\n",
    "    for gender, ratio in final_dist['gender'].items():\n",
    "        print(f\"{gender}: {ratio:.1%}\")\n",
    "\n",
    "    print(\"\\nOverall Age Distribution:\")\n",
    "    for age_group, ratio in final_dist['age'].items():\n",
    "        print(f\"{age_group}: {ratio:.1%}\")\n",
    "\n",
    "    # Get temporal patterns\n",
    "    hourly_patterns = demographic_metrics.get_hourly_patterns()\n",
    "    print(\"\\nPeak Activity Periods:\")\n",
    "    for hour, stats in hourly_patterns.items():\n",
    "        if stats['total_count'] > 0:\n",
    "            print(f\"\\nHour {hour:02d}:00:\")\n",
    "            print(f\"Total people: {stats['total_count']}\")\n",
    "            print(\"Gender distribution:\",\n",
    "                  {k: f\"{v:.1%}\" for k, v in stats['gender_distribution'].items()})\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of Traffic Patterns and Flow\n",
    "\n",
    "Now that we've covered the basics of tracking and demographics, let's explore how we can analyze \n",
    "the movement patterns of people through our camera network. Understanding traffic flow helps in \n",
    "optimizing space usage and identifying potential bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_traffic_patterns(video_paths: dict, duration_minutes: float = 5.0):\n",
    "    \"\"\"\n",
    "    Analyze traffic patterns across multiple cameras over a specified duration.\n",
    "\n",
    "    This function helps us understand:\n",
    "    - How people move between different areas\n",
    "    - Which paths are most commonly taken\n",
    "    - Peak traffic times and locations\n",
    "    - Potential congestion points\n",
    "    \"\"\"\n",
    "    # Convert duration to frames (assuming 30 fps)\n",
    "    max_frames = int(duration_minutes * 60 * 30)\n",
    "\n",
    "    # Initialize our flow tracking system\n",
    "    flow_metrics = {\n",
    "        'transitions': defaultdict(int),  # Camera-to-camera transitions\n",
    "        'dwell_times': defaultdict(list),  # Time spent in each camera view\n",
    "        # Traffic by location and time\n",
    "        'traffic_density': defaultdict(lambda: defaultdict(int)),\n",
    "        'path_sequences': []  # Complete paths taken by individuals\n",
    "    }\n",
    "\n",
    "    # Process videos\n",
    "    captures = {cam_id: cv2.VideoCapture(path)\n",
    "                for cam_id, path in video_paths.items()}\n",
    "\n",
    "    try:\n",
    "        for frame_idx in tqdm(range(max_frames), desc=\"Analyzing traffic patterns\"):\n",
    "            frames = {}\n",
    "            current_time = datetime.now()\n",
    "\n",
    "            # Process each camera\n",
    "            for cam_id, cap in captures.items():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Detect and track people\n",
    "                detections = detector.detect(frame_rgb)\n",
    "                tracks = tracker.update(frame_rgb, detections)\n",
    "\n",
    "                # Update global tracking\n",
    "                for track in tracks:\n",
    "                    # Get global ID for consistent tracking across cameras\n",
    "                    global_id = matcher.update(\n",
    "                        camera_id=cam_id,\n",
    "                        track_id=track.track_id,\n",
    "                        feature=track.feature,\n",
    "                        timestamp=current_time\n",
    "                    )\n",
    "\n",
    "                    # Record movements and patterns\n",
    "                    _update_flow_metrics(\n",
    "                        flow_metrics, global_id, cam_id, track, current_time)\n",
    "\n",
    "            # Periodically visualize the flow patterns\n",
    "            if frame_idx % 30 == 0:  # Every second\n",
    "                _visualize_flow_patterns(flow_metrics, frames)\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        for cap in captures.values():\n",
    "            cap.release()\n",
    "\n",
    "    return flow_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_flow_metrics(metrics, global_id, camera_id, track, timestamp):\n",
    "    \"\"\"\n",
    "    Update traffic flow metrics with new tracking information.\n",
    "    \"\"\"\n",
    "    # Get previous location of this person\n",
    "    prev_camera = matcher.get_previous_camera(global_id, camera_id)\n",
    "\n",
    "    if prev_camera is not None and prev_camera != camera_id:\n",
    "        # Record camera transition\n",
    "        transition_key = (prev_camera, camera_id)\n",
    "        metrics['transitions'][transition_key] += 1\n",
    "\n",
    "        # Update path sequence\n",
    "        metrics['path_sequences'].append(\n",
    "            (global_id, prev_camera, camera_id, timestamp))\n",
    "\n",
    "    # Record dwell time in current camera\n",
    "    metrics['dwell_times'][camera_id].append({\n",
    "        'global_id': global_id,\n",
    "        'timestamp': timestamp,\n",
    "        'position': track.bbox  # This helps analyze specific areas within the camera view\n",
    "    })\n",
    "\n",
    "    # Update traffic density (divide frame into 5x5 grid)\n",
    "    x1, y1, x2, y2 = track.bbox\n",
    "    center_x = (x1 + x2) // 2\n",
    "    center_y = (y1 + y2) // 2\n",
    "    grid_x = center_x // (frame_width // 5)\n",
    "    grid_y = center_y // (frame_height // 5)\n",
    "    metrics['traffic_density'][camera_id][(grid_x, grid_y)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualize_flow_patterns(metrics, frames):\n",
    "    \"\"\"\n",
    "    Create visualizations of current traffic patterns.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # 1. Traffic Flow Graph\n",
    "    plt.subplot(2, 2, 1)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges with weights based on transition counts\n",
    "    for (cam1, cam2), count in metrics['transitions'].items():\n",
    "        G.add_edge(f\"Cam {cam1}\", f\"Cam {cam2}\", weight=count)\n",
    "\n",
    "    # Draw the graph\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='lightblue',\n",
    "            node_size=1000, font_size=10, font_weight='bold')\n",
    "\n",
    "    edge_weights = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_weights)\n",
    "\n",
    "    plt.title(\"Traffic Flow Between Cameras\")\n",
    "\n",
    "    # 2. Heatmap of Traffic Density\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for cam_id, density in metrics['traffic_density'].items():\n",
    "        density_matrix = np.zeros((5, 5))\n",
    "        for (x, y), count in density.items():\n",
    "            density_matrix[y, x] = count\n",
    "\n",
    "        plt.imshow(density_matrix, cmap='YlOrRd')\n",
    "        plt.colorbar(label='Traffic Count')\n",
    "        plt.title(f\"Traffic Density Heatmap - Camera {cam_id}\")\n",
    "\n",
    "        # Add grid lines\n",
    "        for i in range(6):\n",
    "            plt.axhline(y=i-0.5, color='black', linewidth=1)\n",
    "            plt.axvline(x=i-0.5, color='black', linewidth=1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Behavior Analysis\n",
    "\n",
    "Let's add functionality to analyze various behavioral patterns and interactions.\n",
    "This can help identify unusual activities or understand how people use the space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BehaviorAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes behavioral patterns in surveillance footage.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.interaction_threshold = 100  # pixels\n",
    "        self.stationary_threshold = 50    # pixels\n",
    "        self.time_window = 30             # frames\n",
    "\n",
    "        # Store historical data\n",
    "        self.position_history = defaultdict(\n",
    "            lambda: deque(maxlen=self.time_window))\n",
    "        self.interaction_history = defaultdict(list)\n",
    "        self.behavior_patterns = defaultdict(list)\n",
    "\n",
    "    def analyze_frame(self, tracks, timestamp):\n",
    "        \"\"\"\n",
    "        Analyze behaviors in the current frame.\n",
    "        \"\"\"\n",
    "        # Update position history\n",
    "        current_positions = {}\n",
    "        for track in tracks:\n",
    "            x1, y1, x2, y2 = track.bbox\n",
    "            center = ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "            current_positions[track.track_id] = center\n",
    "            self.position_history[track.track_id].append((center, timestamp))\n",
    "\n",
    "        # Analyze behaviors\n",
    "        behaviors = {\n",
    "            'stationary': self._detect_stationary_people(current_positions),\n",
    "            'interactions': self._detect_interactions(current_positions),\n",
    "            'rapid_movement': self._detect_rapid_movement(current_positions),\n",
    "            'groups': self._detect_groups(current_positions)\n",
    "        }\n",
    "\n",
    "        return behaviors\n",
    "\n",
    "    def _detect_stationary_people(self, current_positions):\n",
    "        \"\"\"\n",
    "        Identify people who have remained relatively still.\n",
    "        \"\"\"\n",
    "        stationary = []\n",
    "        for track_id, history in self.position_history.items():\n",
    "            if len(history) < self.time_window:\n",
    "                continue\n",
    "\n",
    "            # Calculate total movement\n",
    "            positions = [pos for pos, _ in history]\n",
    "            total_movement = sum(np.linalg.norm(np.array(p2) - np.array(p1))\n",
    "                                 for p1, p2 in zip(positions[:-1], positions[1:]))\n",
    "\n",
    "            if total_movement < self.stationary_threshold:\n",
    "                stationary.append(track_id)\n",
    "\n",
    "        return stationary\n",
    "\n",
    "    def _detect_interactions(self, current_positions):\n",
    "        \"\"\"\n",
    "        Detect potential interactions between people based on proximity.\n",
    "        \"\"\"\n",
    "        interactions = []\n",
    "        positions = list(current_positions.items())\n",
    "\n",
    "        for i in range(len(positions)):\n",
    "            for j in range(i + 1, len(positions)):\n",
    "                id1, pos1 = positions[i]\n",
    "                id2, pos2 = positions[j]\n",
    "\n",
    "                distance = np.linalg.norm(np.array(pos1) - np.array(pos2))\n",
    "                if distance < self.interaction_threshold:\n",
    "                    interactions.append((id1, id2))\n",
    "\n",
    "        return interactions\n",
    "\n",
    "    def _detect_rapid_movement(self, current_positions):\n",
    "        \"\"\"\n",
    "        Identify people moving unusually quickly.\n",
    "        \"\"\"\n",
    "        rapid_movers = []\n",
    "        for track_id, history in self.position_history.items():\n",
    "            if len(history) < 2:\n",
    "                continue\n",
    "\n",
    "            # Calculate recent velocity\n",
    "            recent_positions = [pos for pos, _ in history[-2:]]\n",
    "            velocity = np.linalg.norm(np.array(recent_positions[1]) -\n",
    "                                      np.array(recent_positions[0]))\n",
    "\n",
    "            if velocity > self.stationary_threshold * 2:\n",
    "                rapid_movers.append(track_id)\n",
    "\n",
    "        return rapid_movers\n",
    "\n",
    "    def _detect_groups(self, current_positions):\n",
    "        \"\"\"\n",
    "        Identify groups of people using clustering.\n",
    "        \"\"\"\n",
    "        if len(current_positions) < 2:\n",
    "            return []\n",
    "\n",
    "        # Convert positions to array for clustering\n",
    "        positions = np.array(list(current_positions.values()))\n",
    "        track_ids = list(current_positions.keys())\n",
    "\n",
    "        # Use DBSCAN for clustering\n",
    "        clustering = DBSCAN(eps=self.interaction_threshold,\n",
    "                            min_samples=2).fit(positions)\n",
    "\n",
    "        # Organize results into groups\n",
    "        groups = defaultdict(list)\n",
    "        for track_id, label in zip(track_ids, clustering.labels_):\n",
    "            if label >= 0:  # Ignore noise points (-1)\n",
    "                groups[label].append(track_id)\n",
    "\n",
    "        return list(groups.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. System Performance Analysis\n",
    "\n",
    "Let's analyze the performance of our tracking system and identify areas for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_system_performance(video_paths: dict, duration_seconds: int = 30):\n",
    "    \"\"\"\n",
    "    Analyze and report on system performance metrics.\n",
    "    \"\"\"\n",
    "    performance_metrics = {\n",
    "        'processing_times': defaultdict(list),\n",
    "        'detection_confidence': defaultdict(list),\n",
    "        'tracking_consistency': defaultdict(list),\n",
    "        'reid_accuracy': defaultdict(list),\n",
    "        'gpu_utilization': [],\n",
    "        'memory_usage': []\n",
    "    }\n",
    "\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "    max_frames = duration_seconds * 30  # Assuming 30 fps\n",
    "\n",
    "    # Process videos\n",
    "    captures = {cam_id: cv2.VideoCapture(path)\n",
    "                for cam_id, path in video_paths.items()}\n",
    "\n",
    "    try:\n",
    "        while frame_count < max_frames:\n",
    "            frame_start = time.time()\n",
    "\n",
    "            for cam_id, cap in captures.items():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    continue\n",
    "\n",
    "                # Measure detection time\n",
    "                det_start = time.time()\n",
    "                detections = detector.detect(frame)\n",
    "                det_time = time.time() - det_start\n",
    "                performance_metrics['processing_times']['detection'].append(\n",
    "                    det_time)\n",
    "\n",
    "                # Record detection confidences\n",
    "                if detections:\n",
    "                    confidences = [conf for _, conf in detections]\n",
    "                    performance_metrics['detection_confidence'][cam_id].extend(\n",
    "                        confidences)\n",
    "\n",
    "                # Measure tracking time\n",
    "                track_start = time.time()\n",
    "                tracks = tracker.update(frame, detections)\n",
    "                track_time = time.time() - track_start\n",
    "                performance_metrics['processing_times']['tracking'].append(\n",
    "                    track_time)\n",
    "\n",
    "                # Measure ReID time\n",
    "                reid_start = time.time()\n",
    "                for track in tracks:\n",
    "                    _ = matcher.update(cam_id, track.track_id,\n",
    "                                       track.feature, datetime.now())\n",
    "                reid_time = time.time() - reid_start\n",
    "                performance_metrics['processing_times']['reid'].append(\n",
    "                    reid_time)\n",
    "\n",
    "            # Record resource usage\n",
    "            if torch.cuda.is_available():\n",
    "                performance_metrics['gpu_utilization'].append(\n",
    "                    torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated()\n",
    "                )\n",
    "\n",
    "            performance_metrics['memory_usage'].append(\n",
    "                psutil.Process().memory_info().rss / (1024 * 1024)  # MB\n",
    "            )\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "    finally:\n",
    "        for cap in captures.values():\n",
    "            cap.release()\n",
    "\n",
    "    # Calculate and display performance statistics\n",
    "    total_time = time.time() - start_time\n",
    "    fps = frame_count / total_time\n",
    "\n",
    "    print(\"\\nSystem Performance Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"Average FPS: {fps:.2f}\")\n",
    "    print(f\"Total processing time: {total_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nProcessing Times (ms):\")\n",
    "    for component, times in performance_metrics['processing_times'].items():\n",
    "        avg_time = np.mean(times) * 1000\n",
    "        std_time = np.std(times) * 1000\n",
    "        print(f\"{component:>10}: {avg_time:>8.2f} ± {std_time:>6.2f}\")\n",
    "\n",
    "    print(\"\\nDetection Quality:\")\n",
    "    for cam_id, confidences in performance_metrics['detection_confidence'].items():\n",
    "        print(f\"Camera {cam_id}:\")\n",
    "        print(f\"  Average confidence: {np.mean(confidences):.3f}\")\n",
    "        print(f\"  Min confidence: {np.min(confidences):.3f}\")\n",
    "        print(f\"  Max confidence: {np.max(confidences):.3f}\")\n",
    "\n",
    "    if performance_metrics['gpu_utilization']:\n",
    "        print(\"\\nResource Usage:\")\n",
    "        print(\n",
    "            f\"Average GPU utilization: {np.mean(performance_metrics['gpu_utilization']):.1%}\")\n",
    "        print(\n",
    "            f\"Peak GPU utilization: {np.max(performance_metrics['gpu_utilization']):.1%}\")\n",
    "        print(\n",
    "            f\"Average memory usage: {np.mean(performance_metrics['memory_usage']):.1f} MB\")\n",
    "        print(\n",
    "            f\"Peak memory usage: {np.max(performance_metrics['memory_usage']):.1f} MB\")\n",
    "\n",
    "    # Visualize performance metrics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Processing times\n",
    "    plt.subplot(2, 2, 1)\n",
    "    times_df = pd.DataFrame(performance_metrics['processing_times'])\n",
    "    times_df.boxplot()\n",
    "    plt.title('Processing Times Distribution')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "\n",
    "    # Detection confidence histogram\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for cam_id, confidences in performance_metrics['detection_confidence'].items():\n",
    "        plt.hist(confidences, alpha=0.5, label=f'Camera {cam_id}')\n",
    "    plt.title('Detection Confidence Distribution')\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    # GPU utilization over time\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(performance_metrics['gpu_utilization'])\n",
    "    plt.title('GPU Utilization Over Time')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Utilization %')\n",
    "\n",
    "    # Memory usage over time\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(performance_metrics['memory_usage'])\n",
    "    plt.title('Memory Usage Over Time')\n",
    "    plt.xlabel('Frame')\n",
    "    plt.ylabel('Memory Usage (MB)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Optimization Guidelines\n",
    "\n",
    "Now that we've explored the system's capabilities and analyzed its performance, let's discuss\n",
    "best practices for deploying and optimizing the CCTV analysis system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_optimization_techniques():\n",
    "    \"\"\"\n",
    "    Demonstrate various optimization techniques and their impact on system performance.\n",
    "    \"\"\"\n",
    "    print(\"Optimization Techniques for CCTV Analysis System\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\n1. Camera Setup Optimization:\")\n",
    "    print(\"   - Optimal camera placement height: 2.5-3.5 meters\")\n",
    "    print(\"   - Recommended camera angle: 15-30 degrees downward\")\n",
    "    print(\"   - Minimum resolution: 1080p for accurate detection\")\n",
    "    print(\"   - Frame rate: 15-30 fps depending on scenario\")\n",
    "\n",
    "    print(\"\\n2. Detection Optimization:\")\n",
    "    print(\"   - Batch processing for multiple cameras\")\n",
    "    print(\"   - Adaptive confidence thresholds\")\n",
    "    print(\"   - Region of Interest (ROI) masking\")\n",
    "\n",
    "    print(\"\\n3. Tracking Optimization:\")\n",
    "    print(\"   - Motion prediction for occlusion handling\")\n",
    "    print(\"   - Feature caching for frequent re-identification\")\n",
    "    print(\"   - Track pruning for memory efficiency\")\n",
    "\n",
    "    print(\"\\n4. Resource Management:\")\n",
    "    print(\"   - GPU memory optimization\")\n",
    "    print(\"   - Batch size tuning\")\n",
    "    print(\"   - Pipeline parallelization\")\n",
    "\n",
    "    # Example of ROI masking impact\n",
    "    def demonstrate_roi_masking():\n",
    "        \"\"\"Demonstrate the impact of ROI masking on processing time.\"\"\"\n",
    "        frame = cv2.imread(\"sample_frame.jpg\")\n",
    "        if frame is None:\n",
    "            return\n",
    "\n",
    "        height, width = frame.shape[:2]\n",
    "\n",
    "        # Create ROI mask (example: focus on central area)\n",
    "        mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        roi_margin = int(width * 0.2)  # 20% margin from edges\n",
    "        mask[roi_margin:-roi_margin, roi_margin:-roi_margin] = 255\n",
    "\n",
    "        # Compare processing times\n",
    "        times_no_roi = []\n",
    "        times_with_roi = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            # Without ROI\n",
    "            start = time.time()\n",
    "            detector.detect(frame)\n",
    "            times_no_roi.append(time.time() - start)\n",
    "\n",
    "            # With ROI\n",
    "            start = time.time()\n",
    "            masked_frame = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "            detector.detect(masked_frame)\n",
    "            times_with_roi.append(time.time() - start)\n",
    "\n",
    "        print(\"\\nROI Masking Performance Impact:\")\n",
    "        print(\n",
    "            f\"Without ROI: {np.mean(times_no_roi)*1000:.2f}ms ± {np.std(times_no_roi)*1000:.2f}ms\")\n",
    "        print(\n",
    "            f\"With ROI: {np.mean(times_with_roi)*1000:.2f}ms ± {np.std(times_with_roi)*1000:.2f}ms\")\n",
    "        print(\n",
    "            f\"Speed improvement: {(1 - np.mean(times_with_roi)/np.mean(times_no_roi))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Additional Analysis Tools\n",
    "\n",
    "Let's provide some additional tools for analyzing the surveillance data and extracting insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_crowd_dynamics(tracks: List[Track], frame_size: Tuple[int, int]):\n",
    "    \"\"\"\n",
    "    Analyze crowd dynamics and movement patterns.\n",
    "    \"\"\"\n",
    "    frame_height, frame_width = frame_size\n",
    "\n",
    "    # Create grid for density analysis\n",
    "    grid_size = 32  # pixels per grid cell\n",
    "    density_map = np.zeros(\n",
    "        (frame_height // grid_size, frame_width // grid_size))\n",
    "\n",
    "    # Analyze track positions\n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2 = track.bbox\n",
    "        center_x = (x1 + x2) // 2\n",
    "        center_y = (y1 + y2) // 2\n",
    "\n",
    "        # Update density map\n",
    "        grid_x = center_x // grid_size\n",
    "        grid_y = center_y // grid_size\n",
    "        if 0 <= grid_x < density_map.shape[1] and 0 <= grid_y < density_map.shape[0]:\n",
    "            density_map[grid_y, grid_x] += 1\n",
    "\n",
    "    return {\n",
    "        'density_map': density_map,\n",
    "        'hotspots': np.where(density_map > np.mean(density_map) + np.std(density_map)),\n",
    "        'average_density': np.mean(density_map),\n",
    "        'peak_density': np.max(density_map)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_analysis_report(video_paths: dict, analysis_duration: int = 300):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive analysis report including all metrics and insights.\n",
    "    \"\"\"\n",
    "    print(\"Generating Comprehensive Analysis Report\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initialize analysis components\n",
    "    behavior_analyzer = BehaviorAnalyzer()\n",
    "    all_metrics = defaultdict(list)\n",
    "\n",
    "    # Process videos\n",
    "    for cam_id, video_path in video_paths.items():\n",
    "        print(f\"\\nAnalyzing Camera {cam_id}...\")\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        frame_count = 0\n",
    "        while cap.isOpened() and frame_count < analysis_duration:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Process frame\n",
    "            detections = detector.detect(frame)\n",
    "            tracks = tracker.update(frame, detections)\n",
    "\n",
    "            # Analyze behaviors\n",
    "            behaviors = behavior_analyzer.analyze_frame(tracks, datetime.now())\n",
    "\n",
    "            # Analyze crowd dynamics\n",
    "            crowd_analysis = analyze_crowd_dynamics(tracks, frame.shape[:2])\n",
    "\n",
    "            # Update metrics\n",
    "            all_metrics['detections'].append(len(detections))\n",
    "            all_metrics['tracks'].append(len(tracks))\n",
    "            all_metrics['crowd_density'].append(\n",
    "                crowd_analysis['average_density'])\n",
    "            all_metrics['behaviors'].append(behaviors)\n",
    "\n",
    "            frame_count += 1\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "    # Generate report sections\n",
    "    def _generate_crowd_analysis():\n",
    "        avg_density = np.mean(all_metrics['crowd_density'])\n",
    "        peak_density = np.max(all_metrics['crowd_density'])\n",
    "\n",
    "        print(\"\\nCrowd Analysis:\")\n",
    "        print(f\"Average crowd density: {avg_density:.2f} people per cell\")\n",
    "        print(f\"Peak crowd density: {peak_density:.2f} people per cell\")\n",
    "\n",
    "        # Visualize density over time\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(all_metrics['crowd_density'])\n",
    "        plt.title('Crowd Density Over Time')\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Average Density')\n",
    "        plt.show()\n",
    "\n",
    "    def _generate_behavior_analysis():\n",
    "        print(\"\\nBehavior Analysis:\")\n",
    "\n",
    "        # Analyze interaction patterns\n",
    "        all_interactions = []\n",
    "        for behaviors in all_metrics['behaviors']:\n",
    "            all_interactions.extend(behaviors['interactions'])\n",
    "\n",
    "        print(f\"Total interactions detected: {len(all_interactions)}\")\n",
    "\n",
    "        # Analyze group formations\n",
    "        group_sizes = []\n",
    "        for behaviors in all_metrics['behaviors']:\n",
    "            group_sizes.extend([len(group) for group in behaviors['groups']])\n",
    "\n",
    "        if group_sizes:\n",
    "            print(f\"Average group size: {np.mean(group_sizes):.1f} people\")\n",
    "            print(f\"Largest group detected: {np.max(group_sizes)} people\")\n",
    "\n",
    "    # Generate final report\n",
    "    print(\"\\nFinal Analysis Report\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    print(\"\\nOverall Statistics:\")\n",
    "    print(f\"Total frames analyzed: {len(all_metrics['detections'])}\")\n",
    "    print(\n",
    "        f\"Average detections per frame: {np.mean(all_metrics['detections']):.1f}\")\n",
    "    print(f\"Average tracks per frame: {np.mean(all_metrics['tracks']):.1f}\")\n",
    "\n",
    "    _generate_crowd_analysis()\n",
    "    _generate_behavior_analysis()\n",
    "\n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps\n",
    "\n",
    "Our CCTV analysis system provides a comprehensive solution for multi-camera person tracking\n",
    "and analysis. Here are the key takeaways and recommendations for deployment:\n",
    "\n",
    "1. System Capabilities:\n",
    "   - Real-time person detection and tracking\n",
    "   - Cross-camera identity matching\n",
    "   - Demographic analysis\n",
    "   - Behavior and interaction analysis\n",
    "   - Traffic pattern analysis\n",
    "\n",
    "2. Performance Considerations:\n",
    "   - GPU acceleration is crucial for real-time processing\n",
    "   - Batch processing can improve throughput\n",
    "   - ROI masking can reduce computational load\n",
    "   - Memory management is important for long-term stability\n",
    "\n",
    "3. Best Practices:\n",
    "   - Regular system calibration\n",
    "   - Proper camera placement and configuration\n",
    "   - Regular model updates and fine-tuning\n",
    "   - Monitoring of system resources\n",
    "\n",
    "4. Future Improvements:\n",
    "   - Integration of anomaly detection\n",
    "   - Enhanced behavior analysis\n",
    "   - Real-time alerting system\n",
    "   - Custom model training for specific scenarios\n",
    "\n",
    "To get started with your own deployment:\n",
    "1. Configure camera settings according to the optimization guidelines\n",
    "2. Set up the required dependencies using the provided requirements.txt\n",
    "3. Initialize the system components with appropriate parameters\n",
    "4. Monitor and adjust settings based on performance metrics\n",
    "5. Regularly validate the system's accuracy and performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the complete system\n",
    "if __name__ == \"__main__\":\n",
    "    # Define video sources\n",
    "    video_paths = {\n",
    "        1: \"data/videos/camera1.mp4\",\n",
    "        2: \"data/videos/camera2.mp4\",\n",
    "        3: \"data/videos/camera3.mp4\"\n",
    "    }\n",
    "\n",
    "    # Analyze system performance\n",
    "    print(\"Analyzing system performance...\")\n",
    "    performance_metrics = analyze_system_performance(video_paths)\n",
    "\n",
    "    # Generate comprehensive analysis report\n",
    "    print(\"\\nGenerating analysis report...\")\n",
    "    analysis_metrics = generate_analysis_report(video_paths)\n",
    "\n",
    "    print(\"\\nAnalysis complete! Check the visualizations and metrics above for insights.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cctv-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
