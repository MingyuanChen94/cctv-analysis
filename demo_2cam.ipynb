{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import torchreid\n",
    "from datetime import datetime\n",
    "import threading\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import queue\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import csv\n",
    "import math\n",
    "from bytetracker import BYTETracker\n",
    "import torch.nn as nn  # Add this import for neural network components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:52\u001b[1;36m\u001b[0m\n\u001b[1;33m    def extract_date_from_filename(self, filename):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "class PersonTracker:\n",
    "    def __init__(self):\n",
    "        # Initialize logging first\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(\"PersonTracker\")\n",
    "\n",
    "        self.doors = {\n",
    "            'camera1': [(1030, 0), (1700, 560)],\n",
    "            'camera2': [(400, 0), (800, 470)]\n",
    "        }\n",
    "\n",
    "        # Define counting zones\n",
    "        self.counting_zones = {\n",
    "            'camera1': [(1030, 200), (1700, 300)],\n",
    "            'camera2': [(400, 200), (800, 300)]\n",
    "        }\n",
    "\n",
    "        # Initialize models\n",
    "        self.yolo_model = self.initialize_yolo()\n",
    "        self.reid_model = self.initialize_reid()\n",
    "\n",
    "        # Storage for tracked individuals\n",
    "        self.tracked_individuals = {}\n",
    "        self.completed_tracks = set()\n",
    "        self.current_frame_detections = {}\n",
    "        self.camera_appearances = defaultdict(dict)\n",
    "\n",
    "        # Track counts\n",
    "        self.entry_count = 0\n",
    "        self.processed_tracks = set()\n",
    "\n",
    "        # Tracking parameters\n",
    "        self.reid_threshold = 0.92\n",
    "        self.max_track_gap = 1.0\n",
    "        self.min_tracking_frames = 10\n",
    "        self.confidence_threshold = 5\n",
    "\n",
    "        # Add new tracking sets\n",
    "        self.camera1_entries = set()\n",
    "        self.camera1_to_camera2 = set()\n",
    "\n",
    "    def initialize_yolo(self):\n",
    "        \"\"\"Initialize YOLO model\"\"\"\n",
    "        try:\n",
    "            model = YOLO(\"yolov8x.pt\")\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading YOLO model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def initialize_reid(self):\n",
    "        \"\"\"Initialize ReID model\"\"\"\n",
    "        try:\n",
    "            class OSNet(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super(OSNet, self).__init__()\n",
    "                    # Base convolutional layers\n",
    "                    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "                    self.bn1 = nn.BatchNorm2d(64)\n",
    "                    self.relu = nn.ReLU(inplace=True)\n",
    "                    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                    \n",
    "                    # Feature layers\n",
    "                    self.conv2 = nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=1)\n",
    "                    self.bn2 = nn.BatchNorm2d(256)\n",
    "                    self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "                    self.bn3 = nn.BatchNorm2d(512)\n",
    "                    \n",
    "                    # Global average pooling\n",
    "                    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "                    \n",
    "                    # Feature dimension\n",
    "                    self.feat_dim = 512\n",
    "\n",
    "                def forward(self, x):\n",
    "                    x = self.conv1(x)\n",
    "                    x = self.bn1(x)\n",
    "                    x = self.relu(x)\n",
    "                    x = self.maxpool(x)\n",
    "\n",
    "                    x = self.conv2(x)\n",
    "                    x = self.bn2(x)\n",
    "                    x = self.relu(x)\n",
    "\n",
    "                    x = self.conv3(x)\n",
    "                    x = self.bn3(x)\n",
    "                    x = self.relu(x)\n",
    "\n",
    "                    x = self.avgpool(x)\n",
    "                    x = x.view(x.size(0), -1)\n",
    "                    return x\n",
    "\n",
    "            model = OSNet()\n",
    "            model.eval()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                model = model.cuda()\n",
    "                self.logger.info(\"ReID model moved to GPU\")\n",
    "            else:\n",
    "                self.logger.info(\"Running ReID model on CPU\")\n",
    "\n",
    "            return model\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading ReID model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def extract_reid_features(self, frame, bbox):\n",
    "        \"\"\"Extract ReID features from detected person\"\"\"\n",
    "        try:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            person_img = frame[y1:y2, x1:x2]\n",
    "            if person_img.size == 0:\n",
    "                return None\n",
    "\n",
    "            person_img = cv2.resize(person_img, (128, 256))\n",
    "            person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Normalize image\n",
    "            person_img = person_img.astype(np.float32) / 255.0\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            person_img = (person_img - mean) / std\n",
    "\n",
    "            # Convert to tensor\n",
    "            person_img = torch.from_numpy(person_img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                person_img = person_img.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = self.reid_model(person_img)\n",
    "                features = features.cpu().numpy()\n",
    "\n",
    "            if features is None or features.size == 0:\n",
    "                return None\n",
    "\n",
    "            features = features / np.linalg.norm(features)\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting ReID features: {e}\")\n",
    "            return None\n",
    "\n",
    "    def is_in_door_area(self, bbox, camera_id):\n",
    "        \"\"\"Check if detection is in door area\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        door_coords = self.doors[camera_id]\n",
    "        \n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        \n",
    "        door_x1, door_y1 = door_coords[0]\n",
    "        door_x2, door_y2 = door_coords[1]\n",
    "        \n",
    "        return (door_x1 <= center_x <= door_x2 and\n",
    "                door_y1 <= center_y <= door_y2)\n",
    "\n",
    "    def match_person(self, features, current_time, camera_id):\n",
    "        \"\"\"Match person using ReID features\"\"\"\n",
    "        best_match_id = None\n",
    "        best_match_score = self.reid_threshold\n",
    "\n",
    "        current_features = np.array(features).flatten()\n",
    "        current_features = current_features / np.linalg.norm(current_features)\n",
    "\n",
    "        for person_id, person_info in self.tracked_individuals.items():\n",
    "            if person_id in self.completed_tracks:\n",
    "                continue\n",
    "\n",
    "            if (person_info.last_seen is not None and\n",
    "                    current_time - person_info.last_seen > self.max_track_gap):\n",
    "                continue\n",
    "\n",
    "            for stored_feat in person_info.features:\n",
    "                score = np.dot(current_features, stored_feat)\n",
    "                if score > best_match_score:\n",
    "                    best_match_score = score\n",
    "                    best_match_id = person_id\n",
    "\n",
    "        return best_match_id\n",
    "\n",
    "    def analyze_movement_pattern(self, positions, min_positions=3):\n",
    "        \"\"\"Analyze movement pattern\"\"\"\n",
    "        if len(positions) < min_positions:\n",
    "            return None\n",
    "\n",
    "        movements = []\n",
    "        for i in range(1, len(positions)):\n",
    "            prev_pos = positions[i-1][0]\n",
    "            curr_pos = positions[i][0]\n",
    "            dy = curr_pos[1] - prev_pos[1]\n",
    "            movements.append(dy)\n",
    "\n",
    "        downward_count = sum(1 for dy in movements if dy > 0)\n",
    "        if downward_count >= len(movements) * 0.8:\n",
    "            return 'entering'\n",
    "        \n",
    "        return 'other'\n",
    "\n",
    "    def update_person_info(self, person_id, frame, bbox, camera_id, timestamp, features):\n",
    "        \"\"\"Update person information\"\"\"\n",
    "        if person_id not in self.tracked_individuals:\n",
    "            self.tracked_individuals[person_id] = PersonInfo(person_id)\n",
    "\n",
    "        person_info = self.tracked_individuals[person_id]\n",
    "        person_info.update_features(features.squeeze())\n",
    "        person_info.update_appearance(frame[bbox[1]:bbox[3], bbox[0]:bbox[2]])\n",
    "        \n",
    "        if not person_info.update_position(bbox, timestamp):\n",
    "            return\n",
    "\n",
    "        person_info.last_camera = camera_id\n",
    "        person_info.last_seen = timestamp\n",
    "\n",
    "        if camera_id not in person_info.camera_times:\n",
    "            person_info.camera_times[camera_id] = {\n",
    "                'first': timestamp,\n",
    "                'last': timestamp\n",
    "            }\n",
    "        else:\n",
    "            person_info.camera_times[camera_id]['last'] = timestamp\n",
    "\n",
    "        movement = self.analyze_movement_pattern(person_info.prev_positions)\n",
    "        if movement == 'entering' and not person_info.entry_recorded:\n",
    "            if camera_id == 'camera1':\n",
    "                self.camera1_entries.add(person_id)\n",
    "                person_info.entered_camera1 = True\n",
    "                person_info.camera1_entry_time = timestamp\n",
    "            elif camera_id == 'camera2' and person_info.entered_camera1:\n",
    "                self.camera1_to_camera2.add(person_id)\n",
    "            person_info.entry_recorded = True\n",
    "            self.entry_count += 1\n",
    "\n",
    "    def process_frame(self, frame, camera_id, timestamp):\n",
    "        \"\"\"Process a single frame\"\"\"\n",
    "        if frame is None:\n",
    "            return None\n",
    "\n",
    "        results = self.yolo_model(frame)\n",
    "        self.current_frame_detections = {}\n",
    "\n",
    "        for detection in results[0].boxes.data:\n",
    "            bbox = [int(coord.item()) for coord in detection[:4]]\n",
    "            confidence = float(detection[4].item())\n",
    "            class_id = int(detection[5].item())\n",
    "\n",
    "            if class_id == 0 and confidence > 0.5:\n",
    "                if self.is_in_door_area(bbox, camera_id):\n",
    "                    features = self.extract_reid_features(frame, bbox)\n",
    "                    if features is not None:\n",
    "                        person_id = self.match_person(features, timestamp, camera_id)\n",
    "\n",
    "                        if person_id is None:\n",
    "                            person_id = len(self.tracked_individuals)\n",
    "\n",
    "                        if person_id not in self.current_frame_detections:\n",
    "                            self.update_person_info(\n",
    "                                person_id, frame, bbox, camera_id, timestamp, features)\n",
    "                            self.current_frame_detections[person_id] = bbox\n",
    "\n",
    "                            if person_id not in self.completed_tracks:\n",
    "                                cv2.rectangle(frame, (bbox[0], bbox[1]),\n",
    "                                            (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "                                cv2.putText(frame, f\"ID: {person_id}\",\n",
    "                                          (bbox[0], bbox[1]-10),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        door_coords = self.doors[camera_id]\n",
    "        cv2.rectangle(frame,\n",
    "                     (int(door_coords[0][0]), int(door_coords[0][1])),\n",
    "                     (int(door_coords[1][0]), int(door_coords[1][1])),\n",
    "                     (255, 0, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"Valid Entries: {self.entry_count}\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonInfo:\n",
    "    def __init__(self, person_id):\n",
    "        self.person_id = person_id\n",
    "        self.appearances = []\n",
    "        self.features = []  # Store multiple features\n",
    "        self.prev_positions = []\n",
    "        self.last_position = None\n",
    "        self.last_seen = None\n",
    "        self.last_camera = None\n",
    "\n",
    "        # Entry/Exit tracking\n",
    "        self.entry_recorded = False\n",
    "        self.exit_recorded = False\n",
    "        self.entered_camera1 = False\n",
    "        self.has_exited_camera1 = False\n",
    "        self.camera1_entry_time = None\n",
    "        self.camera1_exit_time = None\n",
    "        self.camera_times = {}  # Store timestamps for each camera\n",
    "\n",
    "    def update_appearance(self, image):\n",
    "        \"\"\"Store appearance image\"\"\"\n",
    "        if image.size > 0:  # Only store valid images\n",
    "            self.appearances.append(image.copy())\n",
    "            if len(self.appearances) > 10:  # Keep last 10 appearances\n",
    "                self.appearances.pop(0)\n",
    "\n",
    "    def update_features(self, new_features):\n",
    "        \"\"\"Store multiple features for better matching\"\"\"\n",
    "        feat = np.array(new_features).flatten()\n",
    "        feat = feat / np.linalg.norm(feat)  # Normalize feature vector\n",
    "        self.features.append(feat)\n",
    "        if len(self.features) > 10:  # Keep last 10 features\n",
    "            self.features.pop(0)\n",
    "\n",
    "    def update_position(self, bbox, timestamp):\n",
    "        \"\"\"Update position with timestamp\"\"\"\n",
    "        center = ((bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2)\n",
    "        self.prev_positions.append((center, timestamp))\n",
    "        if len(self.prev_positions) > 30:  # Keep last 30 positions\n",
    "            self.prev_positions.pop(0)\n",
    "        self.last_position = center\n",
    "        self.last_seen = timestamp\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PersonTracker:ReID model moved to GPU\n",
      "INFO:PersonTracker:\n",
      "Processing videos for date: 20241011\n",
      "INFO:PersonTracker:Processing C:\\Users\\mc1159\\OneDrive - University of Exeter\\Documents\\VISIONARY\\Durham Experiment\\Experiment Data\\Before\\Camera_1_20241011.mp4\n",
      "ERROR:root:Error during tracking: 'PersonTracker' object has no attribute 'process_frame'\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PersonTracker' object has no attribute 'process_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# video_dir = os.path.join('C:\\\\Users', 'mc1159', 'OneDrive - University of Exeter',\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#                          'Documents', 'VISIONARY', 'Durham Experiment', 'test_data')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 9\u001b[0m     \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     results \u001b[38;5;241m=\u001b[39m tracker\u001b[38;5;241m.\u001b[39manalyze_tracks()\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTracking Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 194\u001b[0m, in \u001b[0;36mPersonTracker.process_videos\u001b[1;34m(self, video_dir, output_dir)\u001b[0m\n\u001b[0;32m    192\u001b[0m frame_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    193\u001b[0m timestamp \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mget(cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_POS_MSEC) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m--> 194\u001b[0m processed_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_frame\u001b[49m(frame, camera_id, timestamp)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Show real-time feedback\u001b[39;00m\n\u001b[0;32m    197\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCamera \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcamera_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, processed_frame)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PersonTracker' object has no attribute 'process_frame'"
     ]
    }
   ],
   "source": [
    "tracker = PersonTracker()\n",
    "video_dir = os.path.join('C:\\\\Users', 'mc1159', 'OneDrive - University of Exeter',\n",
    "                         'Documents', 'VISIONARY', 'Durham Experiment', 'Experiment Data', 'Before')\n",
    "\n",
    "# video_dir = os.path.join('C:\\\\Users', 'mc1159', 'OneDrive - University of Exeter',\n",
    "#                          'Documents', 'VISIONARY', 'Durham Experiment', 'test_data')\n",
    "\n",
    "try:\n",
    "    tracker.process_videos(video_dir)\n",
    "    results = tracker.analyze_tracks()\n",
    "\n",
    "    print(\"\\nTracking Results:\")\n",
    "    print(f\"Total unique individuals: {results['total_unique_individuals']}\")\n",
    "    print(\n",
    "        f\"People moving from Camera 1 to Camera 2: {results['camera1_to_camera2_count']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during tracking: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonTracker:\n",
    "    def __init__(self):\n",
    "        # Initialize logging first\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(\"PersonTracker\")\n",
    "\n",
    "        self.doors = {\n",
    "            'camera1': [(1030, 0), (1700, 560)],\n",
    "            'camera2': [(400, 0), (800, 470)]\n",
    "        }\n",
    "\n",
    "        # Define counting zones\n",
    "        self.counting_zones = {\n",
    "            'camera1': [(1030, 200), (1700, 300)],\n",
    "            'camera2': [(400, 200), (800, 300)]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            # Initialize YOLO model\n",
    "            self.yolo_model = YOLO(\"yolov8x.pt\")  # Using YOLOv8 instead of YOLO11\n",
    "            self.logger.info(\"YOLO model initialized successfully\")\n",
    "\n",
    "            # Initialize OSNet for ReID\n",
    "            class OSNet(nn.Module):\n",
    "                def __init__(self):\n",
    "                    super(OSNet, self).__init__()\n",
    "                    # Base convolutional layers\n",
    "                    self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "                    self.bn1 = nn.BatchNorm2d(64)\n",
    "                    self.relu = nn.ReLU(inplace=True)\n",
    "                    self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "                    \n",
    "                    # Feature layers\n",
    "                    self.conv2 = nn.Conv2d(64, 256, kernel_size=3, stride=2, padding=1)\n",
    "                    self.bn2 = nn.BatchNorm2d(256)\n",
    "                    self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "                    self.bn3 = nn.BatchNorm2d(512)\n",
    "                    \n",
    "                    # Global average pooling\n",
    "                    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "                    \n",
    "                    # Feature dimension\n",
    "                    self.feat_dim = 512\n",
    "\n",
    "                def forward(self, x):\n",
    "                    x = self.conv1(x)\n",
    "                    x = self.bn1(x)\n",
    "                    x = self.relu(x)\n",
    "                    x = self.maxpool(x)\n",
    "\n",
    "                    x = self.conv2(x)\n",
    "                    x = self.bn2(x)\n",
    "                    x = self.relu(x)\n",
    "\n",
    "                    x = self.conv3(x)\n",
    "                    x = self.bn3(x)\n",
    "                    x = self.relu(x)\n",
    "\n",
    "                    x = self.avgpool(x)\n",
    "                    x = x.view(x.size(0), -1)\n",
    "                    return x\n",
    "\n",
    "            # Initialize and set up the ReID model\n",
    "            self.reid_model = OSNet()\n",
    "            self.reid_model.eval()  # Set to evaluation mode\n",
    "\n",
    "            # Move to GPU if available\n",
    "            if torch.cuda.is_available():\n",
    "                self.reid_model = self.reid_model.cuda()\n",
    "                self.logger.info(\"ReID model moved to GPU\")\n",
    "            else:\n",
    "                self.logger.info(\"Running ReID model on CPU\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error initializing models: {e}\")\n",
    "            raise\n",
    "\n",
    "        # Storage for tracked individuals\n",
    "        self.tracked_individuals = {}\n",
    "        self.completed_tracks = set()\n",
    "        self.current_frame_detections = {}\n",
    "        self.camera_appearances = defaultdict(dict)\n",
    "\n",
    "        # Track counts\n",
    "        self.entry_count = 0\n",
    "        self.processed_tracks = set()\n",
    "\n",
    "        # Tracking parameters\n",
    "        self.reid_threshold = 0.92\n",
    "        self.max_track_gap = 1.0\n",
    "        self.min_tracking_frames = 10\n",
    "        self.confidence_threshold = 5\n",
    "\n",
    "        # Add new tracking sets\n",
    "        self.camera1_entries = set()\n",
    "        self.camera1_to_camera2 = set()\n",
    "\n",
    "    def extract_reid_features(self, frame, bbox):\n",
    "        \"\"\"Extract ReID features from detected person\"\"\"\n",
    "        try:\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            person_img = frame[y1:y2, x1:x2]\n",
    "            if person_img.size == 0:\n",
    "                return None\n",
    "\n",
    "            person_img = cv2.resize(person_img, (128, 256))\n",
    "            person_img = cv2.cvtColor(person_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Normalize image\n",
    "            person_img = person_img.astype(np.float32) / 255.0\n",
    "            mean = np.array([0.485, 0.456, 0.406])\n",
    "            std = np.array([0.229, 0.224, 0.225])\n",
    "            person_img = (person_img - mean) / std\n",
    "\n",
    "            # Convert to tensor\n",
    "            person_img = torch.from_numpy(person_img).permute(2, 0, 1).unsqueeze(0).float()\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                person_img = person_img.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                features = self.reid_model(person_img)\n",
    "                features = features.cpu().numpy()\n",
    "\n",
    "            if features is None or features.size == 0:\n",
    "                return None\n",
    "\n",
    "            features = features / np.linalg.norm(features)\n",
    "            return features\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error extracting ReID features: {e}\")\n",
    "            return None\n",
    "\n",
    "    def is_in_door_area(self, bbox, camera_id):\n",
    "        \"\"\"Check if detection is in door area\"\"\"\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        door_coords = self.doors[camera_id]\n",
    "        \n",
    "        center_x = (x1 + x2) / 2\n",
    "        center_y = (y1 + y2) / 2\n",
    "        \n",
    "        door_x1, door_y1 = door_coords[0]\n",
    "        door_x2, door_y2 = door_coords[1]\n",
    "        \n",
    "        return (door_x1 <= center_x <= door_x2 and\n",
    "                door_y1 <= center_y <= door_y2)\n",
    "\n",
    "    def match_person(self, features, current_time, camera_id):\n",
    "        \"\"\"Match person using ReID features\"\"\"\n",
    "        best_match_id = None\n",
    "        best_match_score = self.reid_threshold\n",
    "\n",
    "        current_features = np.array(features).flatten()\n",
    "        current_features = current_features / np.linalg.norm(current_features)\n",
    "\n",
    "        for person_id, person_info in self.tracked_individuals.items():\n",
    "            if person_id in self.completed_tracks:\n",
    "                continue\n",
    "\n",
    "            if (person_info.last_seen is not None and\n",
    "                    current_time - person_info.last_seen > self.max_track_gap):\n",
    "                continue\n",
    "\n",
    "            for stored_feat in person_info.features:\n",
    "                score = np.dot(current_features, stored_feat)\n",
    "                if score > best_match_score:\n",
    "                    best_match_score = score\n",
    "                    best_match_id = person_id\n",
    "\n",
    "        return best_match_id\n",
    "\n",
    "    def analyze_movement_pattern(self, positions, min_positions=3):\n",
    "        \"\"\"Analyze movement pattern\"\"\"\n",
    "        if len(positions) < min_positions:\n",
    "            return None\n",
    "\n",
    "        movements = []\n",
    "        for i in range(1, len(positions)):\n",
    "            prev_pos = positions[i-1][0]\n",
    "            curr_pos = positions[i][0]\n",
    "            dy = curr_pos[1] - prev_pos[1]\n",
    "            movements.append(dy)\n",
    "\n",
    "        downward_count = sum(1 for dy in movements if dy > 0)\n",
    "        if downward_count >= len(movements) * 0.8:\n",
    "            return 'entering'\n",
    "        \n",
    "        return 'other'\n",
    "\n",
    "    def update_person_info(self, person_id, frame, bbox, camera_id, timestamp, features):\n",
    "        \"\"\"Update person information\"\"\"\n",
    "        if person_id not in self.tracked_individuals:\n",
    "            self.tracked_individuals[person_id] = PersonInfo(person_id)\n",
    "\n",
    "        person_info = self.tracked_individuals[person_id]\n",
    "        person_info.update_features(features.squeeze())\n",
    "        person_info.update_appearance(frame[bbox[1]:bbox[3], bbox[0]:bbox[2]])\n",
    "        \n",
    "        if not person_info.update_position(bbox, timestamp):\n",
    "            return\n",
    "\n",
    "        person_info.last_camera = camera_id\n",
    "        person_info.last_seen = timestamp\n",
    "\n",
    "        if camera_id not in person_info.camera_times:\n",
    "            person_info.camera_times[camera_id] = {\n",
    "                'first': timestamp,\n",
    "                'last': timestamp\n",
    "            }\n",
    "        else:\n",
    "            person_info.camera_times[camera_id]['last'] = timestamp\n",
    "\n",
    "        movement = self.analyze_movement_pattern(person_info.prev_positions)\n",
    "        if movement == 'entering' and not person_info.entry_recorded:\n",
    "            if camera_id == 'camera1':\n",
    "                self.camera1_entries.add(person_id)\n",
    "                person_info.entered_camera1 = True\n",
    "                person_info.camera1_entry_time = timestamp\n",
    "            elif camera_id == 'camera2' and person_info.entered_camera1:\n",
    "                self.camera1_to_camera2.add(person_id)\n",
    "            person_info.entry_recorded = True\n",
    "            self.entry_count += 1\n",
    "\n",
    "    def process_frame(self, frame, camera_id, timestamp):\n",
    "        \"\"\"Process a single frame\"\"\"\n",
    "        if frame is None:\n",
    "            return None\n",
    "\n",
    "        results = self.yolo_model(frame)\n",
    "        self.current_frame_detections = {}\n",
    "\n",
    "        for detection in results[0].boxes.data:\n",
    "            bbox = [int(coord.item()) for coord in detection[:4]]\n",
    "            confidence = float(detection[4].item())\n",
    "            class_id = int(detection[5].item())\n",
    "\n",
    "            if class_id == 0 and confidence > 0.5:\n",
    "                if self.is_in_door_area(bbox, camera_id):\n",
    "                    features = self.extract_reid_features(frame, bbox)\n",
    "                    if features is not None:\n",
    "                        person_id = self.match_person(features, timestamp, camera_id)\n",
    "\n",
    "                        if person_id is None:\n",
    "                            person_id = len(self.tracked_individuals)\n",
    "\n",
    "                        if person_id not in self.current_frame_detections:\n",
    "                            self.update_person_info(\n",
    "                                person_id, frame, bbox, camera_id, timestamp, features)\n",
    "                            self.current_frame_detections[person_id] = bbox\n",
    "\n",
    "                            if person_id not in self.completed_tracks:\n",
    "                                cv2.rectangle(frame, (bbox[0], bbox[1]),\n",
    "                                            (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "                                cv2.putText(frame, f\"ID: {person_id}\",\n",
    "                                          (bbox[0], bbox[1]-10),\n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        door_coords = self.doors[camera_id]\n",
    "        cv2.rectangle(frame,\n",
    "                     (int(door_coords[0][0]), int(door_coords[0][1])),\n",
    "                     (int(door_coords[1][0]), int(door_coords[1][1])),\n",
    "                     (255, 0, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"Valid Entries: {self.entry_count}\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        return frame\n",
    "\n",
    "    def process_videos(self, video_dir, output_dir=None):\n",
    "        \"\"\"Process videos grouped by date\"\"\"\n",
    "        if output_dir is None:\n",
    "            output_dir = os.path.join(video_dir, 'tracking_results')\n",
    "\n",
    "        videos_by_date = defaultdict(list)\n",
    "        for video_file in Path(video_dir).glob(\"Camera_*_*.mp4\"):\n",
    "            date = self.extract_date_from_filename(video_file)\n",
    "            if date:\n",
    "                videos_by_date[date].append(video_file)\n",
    "\n",
    "        for date, video_files in videos_by_date.items():\n",
    "            self.reset_tracking()\n",
    "            self.logger.info(f\"\\nProcessing videos for date: {date}\")\n",
    "\n",
    "            for video_file in sorted(video_files):\n",
    "                camera_id = \"camera1\" if \"Camera_1\" in str(video_file) else \"camera2\"\n",
    "                self.logger.info(f\"Processing {video_file}\")\n",
    "\n",
    "                cap = cv2.VideoCapture(str(video_file))\n",
    "                if not cap.isOpened():\n",
    "                    self.logger.error(f\"Error opening video file: {video_file}\")\n",
    "                    continue\n",
    "\n",
    "                fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "                frame_count = 0\n",
    "\n",
    "                while cap.isOpened():\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        break\n",
    "\n",
    "                    frame_count += 1\n",
    "                    timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000.0\n",
    "                    processed_frame = self.process_frame(frame, camera_id, timestamp)\n",
    "\n",
    "                    cv2.imshow(f\"Camera {camera_id}\", processed_frame)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "\n",
    "                    if frame_count % 100 == 0:\n",
    "                        self.logger.info(f\"Processed {frame_count} frames from {camera_id}\")\n",
    "\n",
    "                cap.release()\n",
    "                cv2.destroyWindow(f\"Camera {camera_id}\")\n",
    "\n",
    "            if output_dir:\n",
    "                self.save_tracking_data(output_dir, date)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def extract_date_from_filename(self, filename):\n",
    "        \"\"\"Extract date from filename format Camera_X_YYYYMMDD\"\"\"\n",
    "        try:\n",
    "            date_str = str(filename).split('_')[-1].split('.')[0]\n",
    "            return date_str\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def analyze_tracks(self):\n",
    "        \"\"\"Analyze tracking results\"\"\"\n",
    "        results = {\n",
    "            'total_unique_individuals': len(self.tracked_individuals) - len(self.completed_tracks),\n",
    "            'total_entries': self.entry_count,\n",
    "            'camera1_entries': len(self.camera1_entries),\n",
    "            'camera2_entries': len(set(pid for pid, info in self.tracked_individuals.items()\n",
    "                                   if hasattr(info, 'camera_times') and 'camera2' in info.camera_times)),\n",
    "            'camera1_to_camera2_count': len(self.camera1_to_camera2),\n",
    "            'camera1_to_camera2_ids': list(self.camera1_to_camera2),\n",
    "            'transitions': []\n",
    "        }\n",
    "\n",
    "        for pid in self.camera1_to_camera2:\n",
    "            if pid in self.tracked_individuals:\n",
    "                person_info = self.tracked_individuals[pid]\n",
    "                if hasattr(person_info, 'camera_times'):\n",
    "                    camera1_times = person_info.camera_times.get('camera1', {})\n",
    "                    camera2_times = person_info.camera_times.get('camera2', {})\n",
    "                    \n",
    "                    if camera1_times and camera2_times:\n",
    "                        results['transitions'].append({\n",
    "                            'person_id': pid,\n",
    "                            'camera1_exit': camera1_times.get('last'),\n",
    "                            'camera2_entry': camera2_times.get('first')\n",
    "                        })\n",
    "\n",
    "        valid_transitions = [t for t in results['transitions']\n",
    "                           if t['camera1_exit'] is not None and t['camera2_entry'] is not None]\n",
    "        \n",
    "        if valid_transitions:\n",
    "            transit_times = [(t['camera2_entry'] - t['camera1_exit'])\n",
    "                           for t in valid_transitions]\n",
    "            results['average_transit_time'] = sum(transit_times) / len(transit_times)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def reset_tracking(self):\n",
    "        \"\"\"Reset tracking states for new date\"\"\"\n",
    "        self.tracked_individuals.clear()\n",
    "        self.completed_tracks.clear()\n",
    "        self.current_frame_detections.clear()\n",
    "        self.camera1_entries.clear()\n",
    "        self.camera1_to_camera2.clear()\n",
    "        self.entry_count = 0\n",
    "\n",
    "    def save_tracking_data(self, output_dir, date):\n",
    "        \"\"\"Save tracking data to CSV files\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save entries data\n",
    "        entries_file = os.path.join(output_dir, f'entries_{date}.csv')\n",
    "        with open(entries_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Date', 'Person_ID', 'Camera_ID', 'Entry_Time', 'Exit_Time'])\n",
    "            \n",
    "            for person_id, person_info in self.tracked_individuals.items():\n",
    "                if hasattr(person_info, 'camera_times'):\n",
    "                    for camera_id, times in person_info.camera_times.items():\n",
    "                        writer.writerow([\n",
    "                            date,\n",
    "                            person_id,\n",
    "                            camera_id,\n",
    "                            f\"{times.get('first', ''):.2f}\" if times.get('first') else '',\n",
    "                            f\"{times.get('last', ''):.2f}\" if times.get('last') else ''\n",
    "                        ])\n",
    "\n",
    "        # Save transitions data\n",
    "        transitions_file = os.path.join(output_dir, f'camera_transitions_{date}.csv')\n",
    "        with open(transitions_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Date', 'Person_ID', 'Camera1_Exit', 'Camera2_Entry', 'Transit_Time_Seconds'])\n",
    "            \n",
    "            for pid in self.camera1_to_camera2:\n",
    "                person_info = self.tracked_individuals.get(pid)\n",
    "                if person_info and hasattr(person_info, 'camera_times'):\n",
    "                    camera1_times = person_info.camera_times.get('camera1', {})\n",
    "                    camera2_times = person_info.camera_times.get('camera2', {})\n",
    "                    \n",
    "                    camera1_exit = camera1_times.get('last')\n",
    "                    camera2_entry = camera2_times.get('first')\n",
    "                    \n",
    "                    if camera1_exit and camera2_entry:\n",
    "                        transit_time = camera2_entry - camera1_exit\n",
    "                        writer.writerow([\n",
    "                            date,\n",
    "                            pid,\n",
    "                            f\"{camera1_exit:.2f}\",\n",
    "                            f\"{camera2_entry:.2f}\",\n",
    "                            f\"{transit_time:.2f}\"\n",
    "                        ])\n",
    "\n",
    "        # Save summary statistics\n",
    "        summary_file = os.path.join(output_dir, f'tracking_summary_{date}.csv')\n",
    "        with open(summary_file, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['Date', 'Metric', 'Value'])\n",
    "            writer.writerow([date, 'Total_Camera1_Entries', len(self.camera1_entries)])\n",
    "            writer.writerow([\n",
    "                date, \n",
    "                'Total_Camera2_Entries',\n",
    "                len(set(pid for pid, info in self.tracked_individuals.items()\n",
    "                    if hasattr(info, 'camera_times') and 'camera2' in info.camera_times))\n",
    "            ])\n",
    "            writer.writerow([date, 'Camera1_to_Camera2_Transitions', len(self.camera1_to_camera2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(video_dir):\n",
    "    tracker = PersonTracker()\n",
    "\n",
    "    # Get all video files and sort them by date\n",
    "    video_files = sorted([f for f in os.listdir(video_dir)\n",
    "                         if f.startswith(('Camera_1_', 'Camera_2_'))])\n",
    "\n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        # Determine camera ID from filename\n",
    "        camera_id = 'camera1' if video_file.startswith(\n",
    "            'Camera_1_') else 'camera2'\n",
    "\n",
    "        print(f\"\\nProcessing {video_file}...\")\n",
    "        tracker.process_video(video_path, camera_id)\n",
    "\n",
    "    # Save results\n",
    "    tracker.save_person_images('person_images')\n",
    "    stats = tracker.get_statistics()\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\nTracking Statistics:\")\n",
    "    print(f\"Total unique individuals: {stats['total_unique_individuals']}\")\n",
    "    print(f\"Individuals in Camera 1: {stats['camera1_count']}\")\n",
    "    print(f\"Individuals in Camera 2: {stats['camera2_count']}\")\n",
    "    print(\n",
    "        f\"Individuals appearing in both cameras: {stats['camera1_to_camera2']}\")\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PersonTracker:YOLO model initialized successfully\n",
      "INFO:PersonTracker:ReID model moved to GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Camera_1_20241011.mp4...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PersonTracker' object has no attribute 'process_video'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Process videos from both cameras\u001b[39;00m\n\u001b[0;32m      2\u001b[0m video_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmc1159\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOneDrive - University of Exeter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVISIONARY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDurham Experiment\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mprocess_videos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[35], line 15\u001b[0m, in \u001b[0;36mprocess_videos\u001b[1;34m(video_dir)\u001b[0m\n\u001b[0;32m     11\u001b[0m     camera_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m video_file\u001b[38;5;241m.\u001b[39mstartswith(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCamera_1_\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcamera2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvideo_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mtracker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_video\u001b[49m(video_path, camera_id)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n\u001b[0;32m     18\u001b[0m tracker\u001b[38;5;241m.\u001b[39msave_person_images(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperson_images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'PersonTracker' object has no attribute 'process_video'"
     ]
    }
   ],
   "source": [
    "# Process videos from both cameras\n",
    "video_dir = os.path.join('C:\\\\Users', 'mc1159', 'OneDrive - University of Exeter',\n",
    "                         'Documents', 'VISIONARY', 'Durham Experiment', 'test_data')\n",
    "\n",
    "\n",
    "process_videos(video_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cctv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
