{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the working directory\n",
    "working_directory = os.path.join(os.path.expanduser(\"~\"), \"Library\", \"CloudStorage\", \n",
    "                              \"OneDrive-UniversityofExeter\", \"Documents\", \"VISIONARY\", \n",
    "                              \"Durham Experiment\", \"processed_data_3\")\n",
    "os.chdir(working_directory)\n",
    "\n",
    "# Verify the current working directory\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the .mp4 files in the folder\n",
    "mp4_files = list(Path(working_directory).glob(\"*.mp4\"))\n",
    "# Print the .mp4 files without showing the parent directories\n",
    "print([file.name for file in mp4_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the files for Camera_1 and Camera_2\n",
    "camera_1_files = [file for file in mp4_files if file.name.startswith(\"Camera_1_\")]\n",
    "camera_2_files = [file for file in mp4_files if file.name.startswith(\"Camera_2_\")]\n",
    "\n",
    "# Sort the files by date extracted from the filename\n",
    "camera_1_files_sorted = sorted(camera_1_files, key=lambda x: x.stem.split('_')[-1])\n",
    "camera_2_files_sorted = sorted(camera_2_files, key=lambda x: x.stem.split('_')[-1])\n",
    "\n",
    "print(\"Camera 1 files sorted by date:\", [file.name for file in camera_1_files_sorted])\n",
    "print(\"Camera 2 files sorted by date:\", [file.name for file in camera_2_files_sorted])\n",
    "\n",
    "del mp4_files, camera_1_files, camera_2_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def draw_door_area(video_path, save_path):\n",
    "    # Read the first frame of the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to read video\")\n",
    "        return\n",
    "    \n",
    "    # Create a copy of the original frame\n",
    "    original_frame = frame.copy()\n",
    "    drawing_frame = frame.copy()\n",
    "    \n",
    "    # Store the coordinates of the rectangle\n",
    "    door_coords = []\n",
    "    \n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        nonlocal drawing_frame\n",
    "        \n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            # Reset if we already have 2 points\n",
    "            if len(door_coords) == 2:\n",
    "                door_coords.clear()\n",
    "                drawing_frame = original_frame.copy()\n",
    "            \n",
    "            door_coords.append((x, y))\n",
    "            # Draw point for visual feedback\n",
    "            cv2.circle(drawing_frame, (x, y), 3, (0, 255, 0), -1)\n",
    "            \n",
    "            if len(door_coords) == 2:\n",
    "                # Draw rectangle using the two points\n",
    "                cv2.rectangle(drawing_frame, door_coords[0], door_coords[1], (0, 255, 0), 2)\n",
    "                # Add instruction text\n",
    "                cv2.putText(drawing_frame, \"Press 'r' to redraw, Enter to confirm\", \n",
    "                          (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.imshow('Frame', drawing_frame)\n",
    "\n",
    "    # Create window and set mouse callback\n",
    "    cv2.imshow('Frame', drawing_frame)\n",
    "    cv2.setMouseCallback('Frame', mouse_callback)\n",
    "    \n",
    "    # Instructions\n",
    "    print(\"Instructions:\")\n",
    "    print(\"1. Click two points to draw a rectangle\")\n",
    "    print(\"2. Press 'r' to redraw if needed\")\n",
    "    print(\"3. Press Enter to confirm and save\")\n",
    "    print(\"4. Press 'q' to quit without saving\")\n",
    "    \n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Press 'r' to reset\n",
    "        if key == ord('r'):\n",
    "            door_coords.clear()\n",
    "            drawing_frame = original_frame.copy()\n",
    "            cv2.imshow('Frame', drawing_frame)\n",
    "        \n",
    "        # Press Enter to confirm\n",
    "        elif key == 13:  # Enter key\n",
    "            if len(door_coords) == 2:\n",
    "                # Ensure coordinates are in top-left, bottom-right order\n",
    "                x1, y1 = min(door_coords[0][0], door_coords[1][0]), min(door_coords[0][1], door_coords[1][1])\n",
    "                x2, y2 = max(door_coords[0][0], door_coords[1][0]), max(door_coords[0][1], door_coords[1][1])\n",
    "                \n",
    "                coordinates = {\n",
    "                    'top_left': {'x': x1, 'y': y1},\n",
    "                    'bottom_right': {'x': x2, 'y': y2}\n",
    "                }\n",
    "                with open(save_path, 'w') as f:\n",
    "                    json.dump(coordinates, f)\n",
    "                print(f\"\\nDoor coordinates saved to {save_path}:\")\n",
    "                print(f\"Top-left: ({x1}, {y1})\")\n",
    "                print(f\"Bottom-right: ({x2}, {y2})\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please complete the rectangle before confirming\")\n",
    "        \n",
    "        # Press 'q' to quit\n",
    "        elif key == ord('q'):\n",
    "            print(\"\\nQuitting without saving\")\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Usage example\n",
    "video_path = camera_1_files_sorted[0]\n",
    "draw_door_area(video_path, save_path=\"door_coordinates_camera1.json\")\n",
    "\n",
    "#video_path = camera_2_files_sorted[0]\n",
    "#draw_door_area(video_path, save_path=\"door_coordinates_camera2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Door coordinates saved to door_coordinates_camera1.json:\n",
    "        Top-left: (1033, 9)\n",
    "        Bottom-right: (1673, 559)\n",
    "\n",
    "\n",
    "Top-left: (459, 2)\n",
    "Bottom-right: (739, 469)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import urllib.request\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "class PersonTracker:\n",
    "    def __init__(self):\n",
    "        self.model = YOLO('yolov8x.pt')\n",
    "\n",
    "        # Download and load face cascade file\n",
    "        cascade_file = 'haarcascade_frontalface_default.xml'\n",
    "        if not os.path.exists(cascade_file):\n",
    "            url = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
    "            urllib.request.urlretrieve(url, cascade_file)\n",
    "\n",
    "        self.face_cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "        self.unique_persons = []\n",
    "        self.position_history = []\n",
    "        self.track_length = 30\n",
    "\n",
    "        # Tracking parameters\n",
    "        self.similarity_threshold = 0.7\n",
    "        self.position_threshold = 150\n",
    "\n",
    "        # Door zone parameters\n",
    "        self.door_zone = None  # Will be set by user\n",
    "\n",
    "    def process_video(self, video_path):\n",
    "        # Initialize VideoCapture\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Unable to open video file: {video_path}\")\n",
    "            return\n",
    "\n",
    "        frame_count = 0\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                print(\"End of video or failed to read frame.\")\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            print(f\"Processing frame {frame_count}...\")\n",
    "\n",
    "            # Preprocess frame\n",
    "            frame = cv2.resize(frame, (640, 384))\n",
    "\n",
    "            # YOLO Inference\n",
    "            try:\n",
    "                results = self.model(frame)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during inference: {e}\")\n",
    "                break\n",
    "\n",
    "            detections = results[0].boxes.data.cpu().numpy()\n",
    "            print(f\"Detections: {len(detections)}\")\n",
    "\n",
    "            # Draw detections\n",
    "            for det in detections:\n",
    "                x1, y1, x2, y2, conf, cls = det\n",
    "                cv2.rectangle(\n",
    "                    frame,\n",
    "                    (int(x1), int(y1)),\n",
    "                    (int(x2), int(y2)),\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "                cv2.putText(\n",
    "                    frame,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (int(x1), int(y1 - 10)),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.5,\n",
    "                    (0, 255, 0),\n",
    "                    2\n",
    "                )\n",
    "\n",
    "            # Display frame\n",
    "            cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "tracker = PersonTracker()\n",
    "unique_count = tracker.process_video(video_path)\n",
    "print(f\"Number of unique individuals detected: {unique_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonTracker:\n",
    "    def __init__(self, video_path, output_dir=\"tracked_persons\"):\n",
    "        # Initialize YOLO model\n",
    "        self.detector = YOLO(\"yolo11x.pt\")\n",
    "        \n",
    "        # Initialize ReID model\n",
    "        self.reid_model = torchreid.models.build_model(\n",
    "            name='osnet_x1_0',\n",
    "            num_classes=1000,\n",
    "            pretrained=True\n",
    "        )\n",
    "        self.reid_model = self.reid_model.cuda() if torch.cuda.is_available() else self.reid_model\n",
    "        self.reid_model.eval()\n",
    "        \n",
    "        # Initialize video capture\n",
    "        self.cap = cv2.VideoCapture(video_path)\n",
    "        self.frame_width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        self.frame_height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        self.fps = int(self.cap.get(cv2.CAP_PROP_FPS))\n",
    "        \n",
    "        # Create output directory\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        self.active_tracks = {}  # Currently active tracks\n",
    "        self.person_features = {}  # Historical features for each ID\n",
    "        self.person_timestamps = {}  # Timestamp information\n",
    "        self.next_id = 0\n",
    "        \n",
    "        # Tracking parameters\n",
    "        self.similarity_threshold = 0.7\n",
    "        self.max_disappeared = self.fps * 2  # Max frames to keep track without detection\n",
    "        self.min_detection_confidence = 0.5\n",
    "        self.feature_weight = 0.4   # Weight for ReID features in matching\n",
    "        self.position_weight = 0.3  # Weight for absolute position (IoU)\n",
    "        self.motion_weight = 0.3    # Weight for relative motion prediction\n",
    "        \n",
    "    def extract_features(self, person_crop):\n",
    "        \"\"\"Extract ReID features from person crop\"\"\"\n",
    "        try:\n",
    "            # Preprocess image for ReID\n",
    "            img = cv2.resize(person_crop, (128, 256))\n",
    "            img = torch.from_numpy(img).float()\n",
    "            img = img.permute(2, 0, 1).unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "                \n",
    "            # Extract features\n",
    "            with torch.no_grad():\n",
    "                features = self.reid_model(img)\n",
    "            return features.cpu().numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting features: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def calculate_box_center(self, box):\n",
    "        \"\"\"Calculate center point of a bounding box\"\"\"\n",
    "        return [(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]\n",
    "\n",
    "    def calculate_velocity(self, current_box, previous_box):\n",
    "        \"\"\"Calculate velocity vector between two boxes\"\"\"\n",
    "        current_center = self.calculate_box_center(current_box)\n",
    "        previous_center = self.calculate_box_center(previous_box)\n",
    "        return [current_center[0] - previous_center[0], \n",
    "                current_center[1] - previous_center[1]]\n",
    "\n",
    "    def predict_next_position(self, box, velocity):\n",
    "        \"\"\"Predict next position based on current position and velocity\"\"\"\n",
    "        center = self.calculate_box_center(box)\n",
    "        predicted_center = [center[0] + velocity[0], center[1] + velocity[1]]\n",
    "        width = box[2] - box[0]\n",
    "        height = box[3] - box[1]\n",
    "        return [predicted_center[0] - width/2, predicted_center[1] - height/2,\n",
    "                predicted_center[0] + width/2, predicted_center[1] + height/2]\n",
    "\n",
    "    def calculate_motion_similarity(self, current_boxes, tracked_boxes, tracked_velocities):\n",
    "        \"\"\"Calculate motion-based similarity\"\"\"\n",
    "        n_detections = len(current_boxes)\n",
    "        n_tracks = len(tracked_boxes)\n",
    "        motion_sim = np.zeros((n_detections, n_tracks))\n",
    "        \n",
    "        for i, current_box in enumerate(current_boxes):\n",
    "            current_center = self.calculate_box_center(current_box)\n",
    "            for j, (tracked_box, velocity) in enumerate(zip(tracked_boxes, tracked_velocities)):\n",
    "                # Predict where the tracked box should be\n",
    "                predicted_box = self.predict_next_position(tracked_box, velocity)\n",
    "                predicted_center = self.calculate_box_center(predicted_box)\n",
    "                \n",
    "                # Calculate distance between prediction and actual position\n",
    "                distance = np.sqrt(\n",
    "                    (current_center[0] - predicted_center[0])**2 +\n",
    "                    (current_center[1] - predicted_center[1])**2\n",
    "                )\n",
    "                # Convert distance to similarity (closer = more similar)\n",
    "                motion_sim[i, j] = np.exp(-distance / 100.0)  # 100 is a scaling factor\n",
    "                \n",
    "        return motion_sim\n",
    "\n",
    "    def detect_occlusion(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Detect if box1 is occluded by box2.\n",
    "        Returns: \n",
    "            - is_occluded (bool): True if box1 is occluded by box2\n",
    "            - occlusion_score (float): Degree of occlusion (0 to 1)\n",
    "        \"\"\"\n",
    "        # Calculate IoU\n",
    "        iou = self.calculate_iou(box1, box2)\n",
    "        \n",
    "        # Calculate centers and areas\n",
    "        center1 = self.calculate_box_center(box1)\n",
    "        center2 = self.calculate_box_center(box2)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        \n",
    "        # Calculate vertical position (y-coordinate)\n",
    "        y1 = box1[3]  # bottom of box1\n",
    "        y2 = box2[3]  # bottom of box2\n",
    "        \n",
    "        # Factors that suggest box1 is behind box2:\n",
    "        # 1. Significant overlap\n",
    "        overlap_factor = 1.0 if iou > 0.3 else 0.0\n",
    "        \n",
    "        # 2. Box2 is closer to camera (generally larger and lower in frame)\n",
    "        size_factor = 1.0 if area2 > area1 else 0.0\n",
    "        position_factor = 1.0 if y2 > y1 else 0.0\n",
    "        \n",
    "        # 3. Box1 is partially contained within box2\n",
    "        contained_horizontally = (\n",
    "            (box1[0] > box2[0] and box1[0] < box2[2]) or\n",
    "            (box1[2] > box2[0] and box1[2] < box2[2])\n",
    "        )\n",
    "        contained_vertically = (\n",
    "            (box1[1] > box2[1] and box1[1] < box2[3]) or\n",
    "            (box1[3] > box2[1] and box1[3] < box2[3])\n",
    "        )\n",
    "        containment_factor = 1.0 if (contained_horizontally and contained_vertically) else 0.0\n",
    "        \n",
    "        # Calculate occlusion score (weighted combination of factors)\n",
    "        occlusion_score = (\n",
    "            0.4 * overlap_factor +\n",
    "            0.2 * size_factor +\n",
    "            0.2 * position_factor +\n",
    "            0.2 * containment_factor\n",
    "        )\n",
    "        \n",
    "        # Determine if occluded based on score threshold\n",
    "        is_occluded = occlusion_score > 0.5\n",
    "        \n",
    "        return is_occluded, occlusion_score\n",
    "\n",
    "    def calculate_similarity_matrix(self, current_features, current_boxes, tracked_features, tracked_boxes):\n",
    "        \"\"\"Calculate similarity matrix combining appearance, position, and motion\"\"\"\n",
    "        n_detections = len(current_features)\n",
    "        n_tracks = len(tracked_features)\n",
    "        \n",
    "        if n_detections == 0 or n_tracks == 0:\n",
    "            return np.array([])\n",
    "            \n",
    "        # Calculate appearance similarity\n",
    "        appearance_sim = 1 - distance.cdist(\n",
    "            np.array([f.flatten() for f in current_features]), \n",
    "            np.array([f.flatten() for f in tracked_features]), \n",
    "            metric='cosine'\n",
    "        )\n",
    "        \n",
    "        # Calculate position similarity using IoU\n",
    "        position_sim = np.zeros((n_detections, n_tracks))\n",
    "        for i, box1 in enumerate(current_boxes):\n",
    "            for j, box2 in enumerate(tracked_boxes):\n",
    "                position_sim[i, j] = self.calculate_iou(box1, box2)\n",
    "        \n",
    "        # Calculate velocities for tracked objects\n",
    "        tracked_velocities = []\n",
    "        for track_id in list(self.active_tracks.keys())[:n_tracks]:\n",
    "            if 'previous_box' in self.active_tracks[track_id]:\n",
    "                velocity = self.calculate_velocity(\n",
    "                    self.active_tracks[track_id]['box'],\n",
    "                    self.active_tracks[track_id]['previous_box']\n",
    "                )\n",
    "            else:\n",
    "                velocity = [0, 0]  # No velocity for new tracks\n",
    "            tracked_velocities.append(velocity)\n",
    "        \n",
    "        # Calculate motion similarity\n",
    "        motion_sim = self.calculate_motion_similarity(current_boxes, tracked_boxes, tracked_velocities)\n",
    "        \n",
    "        # Combine all similarities\n",
    "        similarity_matrix = (\n",
    "            self.feature_weight * appearance_sim + \n",
    "            self.position_weight * position_sim +\n",
    "            self.motion_weight * motion_sim\n",
    "        )\n",
    "        \n",
    "        return similarity_matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_iou(box1, box2):\n",
    "        \"\"\"Calculate IoU between two boxes\"\"\"\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        \n",
    "        intersection = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union = area1 + area2 - intersection\n",
    "        \n",
    "        return intersection / (union + 1e-6)\n",
    "    \n",
    "    def update_feature_history(self, track_id, features):\n",
    "        \"\"\"Maintain rolling window of recent features\"\"\"\n",
    "        self.appearance_history[track_id].append(features)\n",
    "        if len(self.appearance_history[track_id]) > self.max_history_length:\n",
    "            self.appearance_history[track_id].pop(0)\n",
    "            \n",
    "        # Update feature representation using exponential moving average\n",
    "        if track_id in self.person_features:\n",
    "            alpha = 0.7  # Weight for historical features\n",
    "            current_features = self.person_features[track_id]\n",
    "            updated_features = alpha * current_features + (1 - alpha) * features\n",
    "            self.person_features[track_id] = updated_features\n",
    "        else:\n",
    "            self.person_features[track_id] = features\n",
    "\n",
    "    def recover_lost_tracklet(self, features, current_box, frame_time):\n",
    "        \"\"\"Attempt to recover lost tracks\"\"\"\n",
    "        best_match_id = None\n",
    "        best_match_score = 0\n",
    "        \n",
    "        # Check recently lost tracks\n",
    "        lost_tracks_to_remove = []\n",
    "        for lost_id, lost_info in self.lost_tracks.items():\n",
    "            # Skip if lost track is too old\n",
    "            if frame_time - lost_info['last_seen'] > self.max_lost_age:\n",
    "                lost_tracks_to_remove.append(lost_id)\n",
    "                continue\n",
    "                \n",
    "            # Calculate appearance similarity\n",
    "            lost_features = lost_info['features']\n",
    "            appearance_sim = 1 - distance.cosine(features.flatten(), lost_features.flatten())\n",
    "            \n",
    "            # Calculate position similarity based on predicted movement\n",
    "            predicted_box = self.predict_next_position(\n",
    "                lost_info['box'],\n",
    "                lost_info['velocity']\n",
    "            )\n",
    "            position_sim = self.calculate_iou(current_box, predicted_box)\n",
    "            \n",
    "            # Combine similarities\n",
    "            match_score = (\n",
    "                self.feature_weight * appearance_sim +\n",
    "                self.position_weight * position_sim\n",
    "            )\n",
    "            \n",
    "            # Check temporal consistency\n",
    "            if match_score > 0.6 and match_score > best_match_score:\n",
    "                best_match_score = match_score\n",
    "                best_match_id = lost_id\n",
    "        \n",
    "        # Clean up old lost tracks\n",
    "        for lost_id in lost_tracks_to_remove:\n",
    "            del self.lost_tracks[lost_id]\n",
    "            \n",
    "        return best_match_id if best_match_score > 0.6 else None\n",
    "\n",
    "    def update_tracks(self, frame, detections, frame_time):\n",
    "        \"\"\"Update tracks with new detections\"\"\"\n",
    "        current_boxes = []\n",
    "        current_features = []\n",
    "        \n",
    "        # Process new detections\n",
    "        for box, conf in detections:\n",
    "            if conf < self.min_detection_confidence:\n",
    "                continue\n",
    "                \n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            person_crop = frame[y1:y2, x1:x2]\n",
    "            if person_crop.size == 0:\n",
    "                continue\n",
    "                \n",
    "            features = self.extract_features(person_crop)\n",
    "            if features is not None:\n",
    "                current_boxes.append([x1, y1, x2, y2])\n",
    "                current_features.append(features)\n",
    "        \n",
    "        # Get tracked boxes and features\n",
    "        tracked_boxes = []\n",
    "        tracked_features = []\n",
    "        tracked_ids = []\n",
    "        \n",
    "        for track_id, track_info in self.active_tracks.items():\n",
    "            tracked_boxes.append(track_info['box'])\n",
    "            tracked_features.append(track_info['features'])\n",
    "            tracked_ids.append(track_id)\n",
    "        \n",
    "        # Calculate similarity matrix\n",
    "        similarity_matrix = self.calculate_similarity_matrix(\n",
    "            current_features, current_boxes,\n",
    "            tracked_features, tracked_boxes\n",
    "        )\n",
    "        \n",
    "        # Perform matching\n",
    "        matched_indices = []\n",
    "        if similarity_matrix.size > 0:\n",
    "            row_ind, col_ind = linear_sum_assignment(-similarity_matrix)\n",
    "            matched_indices = list(zip(row_ind, col_ind))\n",
    "        \n",
    "        # Process matches\n",
    "        unmatched_detections = []\n",
    "        matched_track_ids = set()\n",
    "        \n",
    "        for detection_idx, track_idx in matched_indices:\n",
    "            similarity = similarity_matrix[detection_idx, track_idx]\n",
    "            if similarity >= self.similarity_threshold:\n",
    "                track_id = tracked_ids[track_idx]\n",
    "                matched_track_ids.add(track_id)\n",
    "                \n",
    "                # Update track\n",
    "                self.active_tracks[track_id].update({\n",
    "                    'previous_box': self.active_tracks[track_id]['box'],\n",
    "                    'box': current_boxes[detection_idx],\n",
    "                    'features': current_features[detection_idx],\n",
    "                    'last_seen': frame_time,\n",
    "                    'disappeared': 0\n",
    "                })\n",
    "                \n",
    "                # Update timestamps\n",
    "                self.person_timestamps[track_id]['last_appearance'] = frame_time\n",
    "                \n",
    "                # Save person image\n",
    "                self.save_person_image(track_id, \n",
    "                    frame[current_boxes[detection_idx][1]:current_boxes[detection_idx][3],\n",
    "                          current_boxes[detection_idx][0]:current_boxes[detection_idx][2]])\n",
    "            else:\n",
    "                unmatched_detections.append(detection_idx)\n",
    "        \n",
    "        # Add unmatched detections as new tracks\n",
    "        for detection_idx in range(len(current_features)):\n",
    "            if not any(detection_idx == m[0] for m in matched_indices):\n",
    "                new_id = self.next_id\n",
    "                self.next_id += 1\n",
    "                \n",
    "                self.active_tracks[new_id] = {\n",
    "                    'state': TrackingState.TENTATIVE,\n",
    "                    'occlusion_counter': 0,\n",
    "                    'box': current_boxes[detection_idx],\n",
    "                    'features': current_features[detection_idx],\n",
    "                    'last_seen': frame_time,\n",
    "                    'disappeared': 0,\n",
    "                    'velocity': [0, 0]  # Initialize velocity for new tracks\n",
    "                }\n",
    "                \n",
    "                self.person_features[new_id] = [current_features[detection_idx]]\n",
    "                self.person_timestamps[new_id] = {\n",
    "                    'first_appearance': frame_time,\n",
    "                    'last_appearance': frame_time\n",
    "                }\n",
    "                \n",
    "                # Save person image\n",
    "                self.save_person_image(new_id, \n",
    "                    frame[current_boxes[detection_idx][1]:current_boxes[detection_idx][3],\n",
    "                          current_boxes[detection_idx][0]:current_boxes[detection_idx][2]])\n",
    "        \n",
    "        # Update disappeared tracks\n",
    "        current_time = frame_time\n",
    "        tracks_to_remove = []\n",
    "        \n",
    "        for track_id in self.active_tracks:\n",
    "            if track_id not in matched_track_ids:\n",
    "                self.active_tracks[track_id]['disappeared'] += 1\n",
    "                if self.active_tracks[track_id]['disappeared'] > self.max_disappeared:\n",
    "                    tracks_to_remove.append(track_id)\n",
    "        \n",
    "        # Remove old tracks\n",
    "        for track_id in tracks_to_remove:\n",
    "            del self.active_tracks[track_id]\n",
    "    \n",
    "    def save_person_image(self, person_id, frame):\n",
    "        \"\"\"Save person image to output directory\"\"\"\n",
    "        person_dir = os.path.join(self.output_dir, f\"person_{person_id}\")\n",
    "        os.makedirs(person_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        cv2.imwrite(os.path.join(person_dir, f\"{timestamp}.jpg\"), frame)\n",
    "    \n",
    "    def process_video(self):\n",
    "        frame_count = 0\n",
    "        \n",
    "        while True:\n",
    "            ret, frame = self.cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            frame_time = frame_count / self.fps\n",
    "            frame_count += 1\n",
    "            \n",
    "            # Detect persons using YOLO\n",
    "            results = self.detector(frame, classes=[0])  # class 0 is person\n",
    "            \n",
    "            # Process detections\n",
    "            detections = []\n",
    "            for result in results:\n",
    "                boxes = result.boxes.cpu().numpy()\n",
    "                for box in boxes:\n",
    "                    detections.append((box.xyxy[0], box.conf[0]))\n",
    "            \n",
    "            # Update tracking\n",
    "            self.update_tracks(frame, detections, frame_time)\n",
    "            \n",
    "            # Visualize results\n",
    "            for track_id, track_info in self.active_tracks.items():\n",
    "                box = track_info['box']\n",
    "                cv2.rectangle(frame, (int(box[0]), int(box[1])), \n",
    "                            (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"ID: {track_id}\", \n",
    "                          (int(box[0]), int(box[1])-10),\n",
    "                          cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # Display frame\n",
    "            cv2.imshow('Tracking', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        return self.generate_report()\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate tracking report\"\"\"\n",
    "        report = {\n",
    "            'total_unique_persons': self.next_id,\n",
    "            'person_details': {}\n",
    "        }\n",
    "        \n",
    "        for person_id in self.person_timestamps.keys():\n",
    "            report['person_details'][person_id] = {\n",
    "                'first_appearance': self.person_timestamps[person_id]['first_appearance'],\n",
    "                'last_appearance': self.person_timestamps[person_id]['last_appearance'],\n",
    "                'duration': self.person_timestamps[person_id]['last_appearance'] - \n",
    "                          self.person_timestamps[person_id]['first_appearance'],\n",
    "                'image_path': os.path.join(self.output_dir, f\"person_{person_id}\")\n",
    "            }\n",
    "            \n",
    "        return report\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cctv-analysis-cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
